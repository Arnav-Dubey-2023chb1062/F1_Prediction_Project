# -*- coding: utf-8 -*-
"""MonacoGP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EOkPRP3sT-KbCuBmn9DlEEUEBVrgq5ya
"""

!pip install fastf1 pandas numpy scikit-learn matplotlib seaborn requests optuna lightgbm prophet feature-engine catboost mlFlow bayesian_optimisation

import os
import fastf1
import pandas as pd
import numpy as np
import requests
import optuna
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
from prophet import Prophet
from tensorflow import keras
import mlflow
import shap
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from feature_engine.selection import SmartCorrelatedSelection
from feature_engine.creation import CyclicalFeatures
import matplotlib.pyplot as plt
import seaborn as sns

# MLflow setup
mlflow.set_tracking_uri("file:./mlruns")
mlflow.set_experiment("F1_Monaco_Predictions")

class TireDegradationAnalyzer:
    def __init__(self):
        self.compound_deg_rates = {
            'SOFT': {'initial': 0.15, 'cliff': 25},
            'MEDIUM': {'initial': 0.10, 'cliff': 35},
            'HARD': {'initial': 0.07, 'cliff': 45}
        }
        self.track_severity = 0.8  # Monaco is harsh on tires (0-1 scale)

    def calculate_degradation(self, compound, lap_number, track_temp, driving_style):
        """Calculate tire degradation based on multiple factors"""
        base_deg = self.compound_deg_rates[compound]['initial']
        cliff_point = self.compound_deg_rates[compound]['cliff']

        # Temperature impact (higher temp = faster degradation)
        temp_factor = 1 + (track_temp - 40) * 0.02 if track_temp > 40 else 1

        # Driving style impact (aggressive = faster degradation)
        style_factor = 1 + (driving_style * 0.3)

        # Progressive degradation with cliff effect
        if lap_number > cliff_point:
            cliff_multiplier = 1 + 0.2 * (lap_number - cliff_point)
        else:
            cliff_multiplier = 1

        # Monaco-specific factor (tight corners, lots of traction zones)
        monaco_factor = 1 + (self.track_severity * 0.2)

        degradation = (base_deg * lap_number * temp_factor * style_factor *
                      cliff_multiplier * monaco_factor)

        return min(degradation, 1.0)  # Cap at 100% degradation

    def estimate_optimal_pitstop(self, compound, track_temp, driving_style):
        """Estimate optimal pit stop lap based on degradation"""
        degradation = 0
        lap = 1

        while degradation < 0.7:  # 70% degradation threshold
            degradation = self.calculate_degradation(
                compound, lap, track_temp, driving_style)
            lap += 1

        return lap - 1

class MonacoGPPredictor:
    def __init__(self):
        self.setup_cache()
        self.scalers = {
            'standard': StandardScaler(),
            'robust': RobustScaler()
        }
        self.models = {
            'lightgbm': None,
            'xgboost': None,
            'catboost': None,
            'neural_net': None,
            'qualifying_boost': None,
            'tire_model': None  # New tire degradation specific model
        }

        # Monaco-specific constants
        self.TRACK_SECTORS = {
            'S1': ['Turn1', 'Casino'],
            'S2': ['Mirabeau', 'Hairpin', 'Portier'],
            'S3': ['Tunnel', 'Chicane', 'Swimming Pool', 'Rascasse']
        }
        self.QUALIFYING_IMPORTANCE = 0.8
        self.tire_analyzer = TireDegradationAnalyzer()

        # Tire compound characteristics
        self.TIRE_COMPOUNDS = {
            'SOFT': {'peak_grip': 1.0, 'optimal_temp': 90},
            'MEDIUM': {'peak_grip': 0.95, 'optimal_temp': 85},
            'HARD': {'peak_grip': 0.90, 'optimal_temp': 80}
        }

    def setup_cache(self):
        os.makedirs("f1_cache", exist_ok=True)
        fastf1.Cache.enable_cache("f1_cache")

    def create_monaco_neural_network(self, input_shape):
        """Specialized neural network for Monaco's unique characteristics"""
        model = keras.Sequential([
            keras.layers.Dense(256, activation='relu', input_shape=input_shape),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.4),
            keras.layers.Dense(128, activation='relu'),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.3),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.BatchNormalization(),
            keras.layers.Dense(32, activation='relu'),
            keras.layers.Dense(1)
        ])
        model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                     loss='huber',  # More robust to outliers
                     metrics=['mae'])
        return model

    def create_tire_specific_features(self, data):
        """Create tire-specific features for Monaco"""

        # Calculate tire age effect
        data['TireAgeEffect'] = data.apply(
            lambda x: self.tire_analyzer.calculate_degradation(
                x['Compound'],
                x['TyreLife'],
                x['TrackTemp'],
                self._estimate_driving_style(x)
            ), axis=1
        )

        # Optimal temperature windows
        data['TireTempDelta'] = data.apply(
            lambda x: abs(x['TrackTemp'] -
                self.TIRE_COMPOUNDS[x['Compound']]['optimal_temp']), axis=1
        )

        # Grip level estimates
        data['EstimatedGrip'] = data.apply(
            lambda x: self._calculate_grip_level(x), axis=1
        )

        # Tire strategy features
        data['PredictedPitWindow'] = data.apply(
            lambda x: self.tire_analyzer.estimate_optimal_pitstop(
                x['Compound'],
                x['TrackTemp'],
                self._estimate_driving_style(x)
            ), axis=1
        )

        # Sector-specific tire wear
        for sector in ['S1', 'S2', 'S3']:
            data[f'{sector}_TireEffect'] = data['TireAgeEffect'] * data[f'{sector}_Performance']

        return data

    def _estimate_driving_style(self, row):
        """Estimate driver's aggressiveness on tires (0-1 scale)"""
        # Higher score = more aggressive
        score = 0

        # Speed vs. sector time correlation
        if 'Speed' in row and 'Sector1Time' in row:
            speed_factor = (row['Speed'] - row['Speed'].mean()) / row['Speed'].std()
            score += 0.3 * min(speed_factor, 1)

        # Historical tire management
        if 'HistoricalTireManagement' in row:
            score += 0.4 * row['HistoricalTireManagement']

        # Current race position factor
        if 'Position' in row:
            position_factor = (20 - row['Position']) / 20
            score += 0.3 * position_factor

        return min(max(score, 0), 1)

    def _calculate_grip_level(self, row):
        """Calculate estimated grip level based on multiple factors"""
        base_grip = self.TIRE_COMPOUNDS[row['Compound']]['peak_grip']

        # Temperature effect
        temp_delta = abs(row['TrackTemp'] -
                        self.TIRE_COMPOUNDS[row['Compound']]['optimal_temp'])
        temp_effect = max(0, 1 - (temp_delta * 0.01))

        # Tire age effect
        age_effect = 1 - row['TireAgeEffect']

        # Track evolution factor (improves with more rubber)
        track_evolution = min(1.1, 1 + (row['LapNumber'] * 0.001))

        return base_grip * temp_effect * age_effect * track_evolution

    def load_historical_data(self):
        try:
            # Load multiple years of Monaco data for better understanding
            all_laps = []
            for year in [2024, 2023, 2022]:
                try:
                    print(f"\nAttempting to load {year} data...")
                    session = fastf1.get_session(year, "Monaco", "R")
                    session.load()

                    # Get basic lap data
                    laps = session.laps.copy()

                    # Add year
                    laps['Year'] = year

                    # Safely convert time columns to seconds
                    time_cols = ['LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time']
                    for col in time_cols:
                        try:
                            if col in laps.columns:
                                laps[f'{col}_seconds'] = laps[col].dt.total_seconds()
                        except Exception as e:
                            print(f"Warning: Could not convert {col} to seconds: {e}")

                    # Safely calculate average speed
                    try:
                        speed_cols = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']
                        available_speed_cols = [col for col in speed_cols if col in laps.columns]
                        if available_speed_cols:
                            speed_data = laps[available_speed_cols].astype(float)
                            laps['Speed'] = speed_data.mean(axis=1)
                    except Exception as e:
                        print(f"Warning: Could not calculate average speed: {e}")

                    # Safely add tire data
                    try:
                        if 'TyreLife' in laps.columns:
                            laps['TyreAge'] = laps['TyreLife']
                            if 'Compound' in laps.columns:
                                laps['CompoundAge'] = laps.groupby('Compound')['TyreLife'].transform('max')
                    except Exception as e:
                        print(f"Warning: Could not process tire data: {e}")

                    # Safely calculate pit stop deltas
                    try:
                        if 'PitInTime' in laps.columns and 'PitOutTime' in laps.columns:
                            mask = ~laps['PitInTime'].isna() & ~laps['PitOutTime'].isna()
                            if mask.any():
                                pit_in = laps.loc[mask, 'PitInTime'].dt.total_seconds()
                                pit_out = laps.loc[mask, 'PitOutTime'].dt.total_seconds()
                                laps.loc[mask, 'PitStopDelta'] = pit_out - pit_in
                    except Exception as e:
                        print(f"Warning: Could not calculate pit stop deltas: {e}")

                    # Only append if we have the minimum required data
                    required_cols = ['Driver', 'LapTime_seconds', 'Position']
                    if all(col in laps.columns for col in required_cols):
                        all_laps.append(laps)
                        print(f"Successfully loaded {year} data with {len(laps)} laps")
                    else:
                        missing = [col for col in required_cols if col not in laps.columns]
                        print(f"Warning: Missing required columns for {year}: {missing}")

                except Exception as e:
                    print(f"Warning: Could not load data for {year}: {e}")
                    print("Available columns:", session.laps.columns.tolist())
                    continue

            if not all_laps:
                raise Exception("No data could be loaded for any year")

            # Combine all years
            laps_data = pd.concat(all_laps, ignore_index=True)
            print(f"\nTotal laps loaded: {len(laps_data)}")
            print("Available columns:", laps_data.columns.tolist())
            return laps_data

        except Exception as e:
            print(f"Error loading Monaco GP data: {str(e)}")
            return None

    def _calculate_monaco_specific_stats(self, data):
        """Calculate Monaco-specific performance statistics"""
        monaco_stats = []

        for driver in data['Driver'].unique():
            driver_data = data[data['Driver'] == driver]

            # Calculate position changes throughout races
            position_changes = []
            for year in [2024, 2023, 2022]:
                year_data = driver_data[driver_data['Year'] == year]
                if not year_data.empty:
                    # Get start and end positions for each stint
                    stints = year_data.groupby('LapNumber')
                    start_pos = stints['Position'].first()
                    end_pos = stints['Position'].last()
                    position_changes.extend(start_pos - end_pos)

            # Calculate overtaking ability
            avg_pos_change = np.mean(position_changes) if position_changes else 0
            pos_change_consistency = np.std(position_changes) if len(position_changes) > 1 else 0

            # Calculate sector performance stability
            sector_stability = {}
            for sector in ['Sector1Time_seconds', 'Sector2Time_seconds', 'Sector3Time_seconds']:
                if sector in driver_data.columns:
                    sector_stability[sector] = driver_data[sector].std()

            # Calculate performance in key Monaco sections
            monaco_sections = {
                'Casino': 'Sector1Time_seconds',
                'Hairpin': 'Sector2Time_seconds',
                'Swimming_Pool': 'Sector3Time_seconds'
            }

            section_performance = {}
            for section, sector in monaco_sections.items():
                if sector in driver_data.columns:
                    section_performance[section] = driver_data[sector].mean()

            # Wet weather performance (if available)
            wet_performance = driver_data[
                driver_data['TrackStatus'].str.contains('WET', na=False)
            ]['LapTime_seconds'].mean() if 'TrackStatus' in driver_data.columns else None

            monaco_stats.append({
                'Driver': driver,
                'AvgPositionGain': avg_pos_change,
                'PositionChangeConsistency': pos_change_consistency,
                'SectorStability': np.mean(list(sector_stability.values())) if sector_stability else None,
                'CasinoPerformance': section_performance.get('Casino'),
                'HairpinPerformance': section_performance.get('Hairpin'),
                'SwimmingPoolPerformance': section_performance.get('Swimming_Pool'),
                'WetPerformance': wet_performance,
                'YearOverYearTrend': self._calculate_performance_trend(driver_data)
            })

        return pd.DataFrame(monaco_stats)

    def _calculate_performance_trend(self, driver_data):
        """Calculate year-over-year performance trend at Monaco"""
        yearly_performances = []

        for year in [2024, 2023, 2022]:
            year_data = driver_data[driver_data['Year'] == year]
            if not year_data.empty:
                # Average lap time for the year
                avg_lap_time = year_data['LapTime_seconds'].mean()
                # Average position for the year
                avg_position = year_data['Position'].mean()
                # Combine into a performance metric
                performance = (1 / avg_lap_time) * (20 - avg_position)
                yearly_performances.append((year, performance))

        if len(yearly_performances) > 1:
            # Calculate trend (positive = improving, negative = declining)
            performances = [p[1] for p in yearly_performances]
            years = [p[0] for p in yearly_performances]

            if len(performances) > 2:
                # Use linear regression for trend
                z = np.polyfit(years, performances, 1)
                trend = z[0]  # Slope indicates trend
            else:
                # Simple difference for two years
                trend = performances[0] - performances[-1]

            return trend

        return 0

    def fetch_monaco_weather(self, api_key):
        """Fetch detailed weather data for Monaco circuit"""
        base_url = "http://api.openweathermap.org/data/2.5"
        lat, lon = 43.7338, 7.4215  # Monaco circuit coordinates

        try:
            # Current weather
            current_url = f"{base_url}/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric"
            current_response = requests.get(current_url)
            current_data = current_response.json()

            # Forecast
            forecast_url = f"{base_url}/forecast?lat={lat}&lon={lon}&appid={api_key}&units=metric"
            forecast_response = requests.get(forecast_url)
            forecast_data = forecast_response.json()

            if 'main' not in current_data or 'temp' not in current_data['main']:
                raise ValueError("Invalid weather data format")

            # Process weather data
            air_temp = current_data['main']['temp']
            humidity = current_data['main']['humidity']

            # Estimate track temperature (typically 10-15°C higher than air temperature in sunny conditions)
            track_temp = air_temp + 12  # Using average offset

            # Get wind data safely
            wind_speed = current_data.get('wind', {}).get('speed', 0)
            wind_direction = current_data.get('wind', {}).get('deg', 0)

            weather_data = {
                'air_temp': air_temp,
                'track_temp': track_temp,
                'humidity': humidity / 100.0,  # Convert to decimal
                'wind_speed': wind_speed,
                'wind_direction': wind_direction,
                'wind_factor': self._calculate_wind_impact({
                    'speed': wind_speed,
                    'deg': wind_direction
                })
            }

            return weather_data

        except requests.RequestException as e:
            print(f"Error making weather API request: {e}")
            return None
        except (KeyError, ValueError) as e:
            print(f"Error fetching Monaco weather data: {e}")
            return None
        except Exception as e:
            print(f"Unexpected error fetching weather data: {e}")
            return None

    def _calculate_wind_impact(self, wind_data):
        """Calculate wind impact specific to Monaco's street circuit"""
        wind_speed = wind_data.get('speed', 0)
        wind_direction = wind_data.get('deg', 0)

        # Monaco-specific wind impact factors
        tunnel_impact = abs(np.cos(np.radians(wind_direction - 45))) * wind_speed
        harbor_impact = abs(np.cos(np.radians(wind_direction - 180))) * wind_speed

        return (tunnel_impact + harbor_impact) / 2

    def preprocess_monaco_data(self, data, weather_data=None):
        """Enhanced data preprocessing specific to Monaco GP"""
        try:
            # Create a copy to avoid modifying the original data
            processed_data = data.copy()

            # Store original driver and team info
            driver_info = None
            team_info = None
            if 'Driver' in processed_data.columns:
                driver_info = processed_data['Driver'].copy()
            if 'Team' in processed_data.columns:
                team_info = processed_data['Team'].copy()

            # First, handle all timedelta columns
            for col in processed_data.columns:
                if pd.api.types.is_timedelta64_dtype(processed_data[col]):
                    try:
                        processed_data[f'{col}_seconds'] = processed_data[col].dt.total_seconds()
                        processed_data = processed_data.drop(col, axis=1)
                    except Exception as e:
                        print(f"Warning: Could not convert {col} to seconds: {e}")

            # Convert datetime columns to timestamp
            datetime_cols = [col for col in processed_data.columns if pd.api.types.is_datetime64_dtype(processed_data[col])]
            for col in datetime_cols:
                try:
                    processed_data[f'{col}_timestamp'] = pd.to_datetime(processed_data[col]).astype(np.int64) // 10**9
                    processed_data = processed_data.drop(col, axis=1)
                except Exception as e:
                    print(f"Warning: Could not convert {col} to timestamp: {e}")

            # Handle boolean columns first (before they might be converted to float)
            bool_cols = ['IsPersonalBest', 'FreshTyre', 'Deleted', 'FastF1Generated', 'IsAccurate']
            for col in bool_cols:
                if col in processed_data.columns:
                    try:
                        # Convert to float first to handle NaN values, then to int
                        processed_data[col] = processed_data[col].astype(float).fillna(-1).astype(int)
                    except Exception as e:
                        print(f"Warning: Could not convert {col} to int: {e}")

            # Encode categorical variables
            categorical_cols = ['Driver', 'Compound', 'Team', 'TrackStatus']
            for col in categorical_cols:
                if col in processed_data.columns:
                    try:
                        # Handle NaN values before encoding
                        processed_data[col] = processed_data[col].fillna('Unknown')
                        processed_data[f'{col}_encoded'] = pd.Categorical(processed_data[col]).codes
                        processed_data = processed_data.drop(col, axis=1)
                    except Exception as e:
                        print(f"Warning: Could not encode {col}: {e}")

            # Handle specific numeric columns
            numeric_cols = ['DriverNumber', 'LapNumber', 'Stint', 'Position', 'Year',
                          'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife']
            for col in numeric_cols:
                if col in processed_data.columns:
                    try:
                        processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')
                    except Exception as e:
                        print(f"Warning: Could not convert {col} to numeric: {e}")

            # Calculate speed features if available
            speed_cols = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']
            available_speed_cols = [col for col in speed_cols if col in processed_data.columns]
            if available_speed_cols:
                try:
                    speed_data = processed_data[available_speed_cols].astype(float)
                    processed_data['MaxSpeed'] = speed_data.max(axis=1)
                    processed_data['MinSpeed'] = speed_data.min(axis=1)
                    processed_data['AvgSpeed'] = speed_data.mean(axis=1)
                    processed_data['SpeedRange'] = processed_data['MaxSpeed'] - processed_data['MinSpeed']
                except Exception as e:
                    print(f"Warning: Could not calculate speed features: {e}")

            # Add weather-related features if available
            if weather_data is not None:
                for key, value in weather_data.items():
                    processed_data[key] = value

            # Convert all remaining columns to float where possible
            for col in processed_data.columns:
                if col not in ['Driver', 'Team'] and not pd.api.types.is_numeric_dtype(processed_data[col]):
                    try:
                        processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')
                    except Exception as e:
                        print(f"Warning: Could not convert {col} to numeric: {e}")

            # Handle missing values for numeric columns
            numeric_cols = processed_data.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if processed_data[col].isnull().any():
                    # For time-based columns, use median to avoid outliers
                    if 'Time' in col or 'time' in col or 'seconds' in col:
                        processed_data[col] = processed_data[col].fillna(processed_data[col].median())
                    # For other numeric columns, use mean
                    else:
                        processed_data[col] = processed_data[col].fillna(processed_data[col].mean())

            # Add back driver and team information if needed
            if driver_info is not None:
                processed_data['Driver'] = driver_info
            if team_info is not None:
                processed_data['Team'] = team_info

            # Final check for any remaining NaN values
            if processed_data.isnull().any().any():
                print("\nWarning: Some NaN values remain in columns:",
                      processed_data.columns[processed_data.isnull().any()].tolist())

            print("\nProcessed features:", processed_data.columns.tolist())
            print("Number of numeric features:", len(numeric_cols))

            return processed_data

        except Exception as e:
            print(f"Error in preprocessing data: {e}")
            print("Available columns:", data.columns.tolist())
            return None

    def _calculate_quali_advantage(self, row):
        """Calculate qualifying advantage specific to Monaco"""
        quali_positions = [row.get(f'QualiPosition_{year}', 20) for year in [2024, 2023, 2022]]
        weights = [0.6, 0.3, 0.1]  # More weight to recent qualifying
        return sum(pos * weight for pos, weight in zip(quali_positions, weights))

    def _calculate_historical_performance(self, row):
        """Calculate historical performance at Monaco with position change analysis"""
        recent_years = [2024, 2023, 2022]
        performances = []
        position_changes = []
        weights = [0.5, 0.3, 0.2]  # More weight to recent performances

        for year, weight in zip(recent_years, weights):
            # Race position
            if f'Position_{year}' in row:
                race_pos = row[f'Position_{year}']
                quali_pos = row.get(f'QualiPosition_{year}')

                if pd.notna(race_pos) and pd.notna(quali_pos):
                    # Calculate position change (positive = positions gained)
                    pos_change = quali_pos - race_pos
                    position_changes.append(pos_change)

                    # Base performance score (21 - position to give higher points for better positions)
                    base_score = 21 - race_pos

                    # Bonus for improving position
                    if pos_change > 0:
                        improvement_bonus = pos_change * 0.5  # Bonus points for each position gained
                    else:
                        improvement_bonus = 0

                    # Final weighted score including position change bonus
                    performances.append(weight * (base_score + improvement_bonus))

        # Calculate position change trend
        if position_changes:
            avg_pos_change = sum(position_changes) / len(position_changes)
            consistency = np.std(position_changes) if len(position_changes) > 1 else 0
        else:
            avg_pos_change = 0
            consistency = 0

        # Return both the performance score and position change metrics
        return {
            'PerformanceScore': sum(performances) if performances else 10,
            'AvgPositionChange': avg_pos_change,
            'PositionChangeConsistency': consistency
        }

    def optimize_lightgbm(self, X_train, y_train, X_val, y_val):
        """Optimize LightGBM hyperparameters using Optuna"""
        def objective(trial):
            # Scale the target variable to help with convergence
            y_scaler = StandardScaler()
            y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()
            y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()

            # Scale features
            X_scaler = StandardScaler()
            X_train_scaled = X_scaler.fit_transform(X_train)
            X_val_scaled = X_scaler.transform(X_val)

            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'verbosity': -1,
                'boosting_type': 'gbdt',
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
                'num_leaves': trial.suggest_int('num_leaves', 20, 100),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'min_split_gain': trial.suggest_float('min_split_gain', 1e-8, 1.0, log=True),
                'feature_pre_filter': False
            }

            # Early stopping callback
            early_stopping = lgb.early_stopping(stopping_rounds=20, verbose=False)

            model = lgb.LGBMRegressor(**params)
            model.fit(
                X_train_scaled, y_train_scaled,
                eval_set=[(X_val_scaled, y_val_scaled)],
                callbacks=[early_stopping]
            )

            # Get predictions and inverse transform
            val_pred = y_scaler.inverse_transform(
                model.predict(X_val_scaled).reshape(-1, 1)
            ).ravel()

            return mean_squared_error(y_val, val_pred)

        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=50, show_progress_bar=True)

        print("\nBest LightGBM parameters:", study.best_params)
        print(f"Best MSE: {study.best_value:.2f}")

        return study.best_params

    def train_monaco_models(self, X_train, y_train, X_val, y_val):
        """Train multiple models with Monaco-specific optimizations"""

        with mlflow.start_run():
            # Scale the data
            X_scaler = StandardScaler()
            y_scaler = StandardScaler()

            X_train_scaled = X_scaler.fit_transform(X_train)
            X_val_scaled = X_scaler.transform(X_val)
            y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()
            y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()

            # LightGBM with Optuna
            print("\nOptimizing LightGBM parameters...")
            lgb_params = self.optimize_lightgbm(X_train, y_train, X_val, y_val)

            # Add early stopping and other parameters
            final_lgb_params = {
                **lgb_params,
                'objective': 'regression',
                'metric': 'rmse',
                'verbosity': -1,
                'boosting_type': 'gbdt',
                'feature_pre_filter': False
            }

            # Train final LightGBM model
            print("\nTraining final LightGBM model...")
            self.models['lightgbm'] = lgb.LGBMRegressor(**final_lgb_params)
            self.models['lightgbm'].fit(
                X_train_scaled, y_train_scaled,
                eval_set=[(X_val_scaled, y_val_scaled)],
                callbacks=[lgb.early_stopping(stopping_rounds=20)]
            )

            # XGBoost with Grid Search
            print("\nTraining XGBoost model...")
            xgb_params = {
                'max_depth': 6,
                'learning_rate': 0.05,
                'n_estimators': 500,
                'min_child_weight': 5,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'objective': 'reg:squarederror'
            }
            self.models['xgboost'] = xgb.XGBRegressor(**xgb_params)
            self.models['xgboost'].fit(X_train_scaled, y_train_scaled)

            # CatBoost
            print("\nTraining CatBoost model...")
            self.models['catboost'] = cb.CatBoostRegressor(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                loss_function='RMSE',
                verbose=False
            )
            self.models['catboost'].fit(X_train_scaled, y_train_scaled)

            # Neural Network
            print("\nTraining Neural Network...")
            input_shape = (X_train.shape[1],)
            self.models['neural_net'] = self.create_monaco_neural_network(input_shape)
            self.models['neural_net'].fit(
                X_train_scaled, y_train_scaled,
                validation_data=(X_val_scaled, y_val_scaled),
                epochs=100,
                batch_size=32,
                verbose=0,
                callbacks=[
                    keras.callbacks.EarlyStopping(
                        monitor='val_loss',
                        patience=10,
                        restore_best_weights=True
                    )
                ]
            )

            # Log metrics for each model
            trained_models = ['lightgbm', 'xgboost', 'catboost', 'neural_net']
            for name in trained_models:
                model = self.models[name]
                if model is not None:  # Extra safety check
                    if name == 'neural_net':
                        train_pred = y_scaler.inverse_transform(
                            model.predict(X_train_scaled).reshape(-1, 1)
                        ).ravel()
                        val_pred = y_scaler.inverse_transform(
                            model.predict(X_val_scaled).reshape(-1, 1)
                        ).ravel()
                    else:
                        train_pred = y_scaler.inverse_transform(
                            model.predict(X_train_scaled).reshape(-1, 1)
                        ).ravel()
                        val_pred = y_scaler.inverse_transform(
                            model.predict(X_val_scaled).reshape(-1, 1)
                        ).ravel()

                    mlflow.log_metrics({
                        f'{name}_train_mae': mean_absolute_error(y_train, train_pred),
                        f'{name}_val_mae': mean_absolute_error(y_val, val_pred),
                        f'{name}_train_r2': r2_score(y_train, train_pred),
                        f'{name}_val_r2': r2_score(y_val, val_pred)
                    })

                    print(f"\n{name} Metrics:")
                    print(f"Train MAE: {mean_absolute_error(y_train, train_pred):.3f}")
                    print(f"Val MAE: {mean_absolute_error(y_val, val_pred):.3f}")
                    print(f"Train R2: {r2_score(y_train, train_pred):.3f}")
                    print(f"Val R2: {r2_score(y_val, val_pred):.3f}")

    def predict_monaco_performance(self, X):
        """Make predictions using Monaco-optimized ensemble"""
        # Scale the input data
        X_scaled = StandardScaler().fit_transform(X)

        predictions = {}
        weights = {
            'lightgbm': 0.35,
            'xgboost': 0.25,
            'catboost': 0.25,
            'neural_net': 0.15
        }

        # Get predictions from each trained model
        trained_models = ['lightgbm', 'xgboost', 'catboost', 'neural_net']
        available_weights_sum = 0

        for name in trained_models:
            model = self.models[name]
            if model is not None:
                if name == 'neural_net':
                    pred = model.predict(X_scaled).ravel()  # Ensure 1D array
                else:
                    pred = model.predict(X_scaled).ravel()  # Ensure 1D array
                predictions[name] = pred
                available_weights_sum += weights[name]

        if available_weights_sum == 0:
            raise ValueError("No trained models available for prediction")

        # Normalize weights for available models
        normalized_weights = {name: weights[name] / available_weights_sum
                            for name in predictions.keys()}

        # Weighted average of predictions
        final_predictions = np.zeros(X.shape[0])  # Initialize with correct shape
        for name, pred in predictions.items():
            final_predictions += pred * normalized_weights[name]

        return final_predictions

    def analyze_monaco_performance(self, X):
        """Analyze feature importance specific to Monaco"""
        # Only analyze if we have a trained LightGBM model
        if self.models['lightgbm'] is not None:
            try:
                # Calculate SHAP values
                explainer = shap.TreeExplainer(self.models['lightgbm'])
                shap_values = explainer.shap_values(X)

                # Create a figure with two subplots side by side
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

                # Plot 1: SHAP Bar Plot
                shap_importance = np.abs(shap_values).mean(0)
                feature_importance = pd.DataFrame({
                    'Feature': X.columns,
                    'Importance': shap_importance
                })
                feature_importance = feature_importance.sort_values('Importance', ascending=True)

                # Plot horizontal bar chart
                sns.barplot(data=feature_importance.tail(15), y='Feature', x='Importance', ax=ax1)
                ax1.set_title('Top 15 Most Important Features (SHAP Values)', fontsize=12, pad=20)
                ax1.set_xlabel('Mean |SHAP Value|')

                # Plot 2: SHAP Summary Plot
                shap.summary_plot(shap_values, X, plot_type="dot", show=False, ax=ax2)
                ax2.set_title('Feature Impact Distribution', fontsize=12, pad=20)

                plt.tight_layout()
                plt.show()

                # Additional feature importance details
                print("\nDetailed Feature Importance Analysis:")
                print("-" * 50)
                for _, row in feature_importance.tail(10).iloc[::-1].iterrows():
                    print(f"{row['Feature']}: {row['Importance']:.4f}")

            except Exception as e:
                print(f"Warning: Could not generate SHAP plot: {e}")
        else:
            print("Warning: LightGBM model not available for feature importance analysis")

        # Additional Monaco-specific visualizations with safety checks
        try:
            self._plot_sector_analysis(X)
        except Exception as e:
            print(f"Warning: Could not generate sector analysis plot: {e}")

        try:
            self._plot_qualifying_impact(X)
        except Exception as e:
            print(f"Warning: Could not generate qualifying impact plot: {e}")

    def _plot_sector_analysis(self, X):
        """Plot enhanced sector-specific performance analysis"""
        # Find sector-related columns
        sector_cols = [col for col in X.columns if 'Sector' in col]
        if not sector_cols:
            print("Warning: No sector-related columns found for visualization")
            return

        # Create a figure with multiple subplots
        fig = plt.figure(figsize=(15, 10))
        gs = plt.GridSpec(2, 2, figure=fig)

        # Plot 1: Sector Time Distribution (Box Plot)
        ax1 = fig.add_subplot(gs[0, :])
        sns.boxplot(data=X[sector_cols], ax=ax1)
        ax1.set_title('Sector Performance Distribution', fontsize=12, pad=10)
        ax1.set_ylabel('Time (seconds)')
        ax1.tick_params(axis='x', rotation=45)

        # Plot 2: Sector Correlation Heatmap
        ax2 = fig.add_subplot(gs[1, 0])
        sector_corr = X[sector_cols].corr()
        sns.heatmap(sector_corr, annot=True, cmap='coolwarm', center=0, ax=ax2)
        ax2.set_title('Sector Correlation', fontsize=12, pad=10)

        # Plot 3: Sector Performance Trends
        ax3 = fig.add_subplot(gs[1, 1])
        sector_means = X[sector_cols].mean().sort_values()
        sector_stds = X[sector_cols].std()

        # Create error bar plot
        ax3.errorbar(
            range(len(sector_means)),
            sector_means,
            yerr=sector_stds,
            fmt='o',
            capsize=5,
            capthick=2,
            elinewidth=2,
            markersize=8
        )
        ax3.set_xticks(range(len(sector_means)))
        ax3.set_xticklabels(sector_means.index, rotation=45)
        ax3.set_title('Average Sector Performance', fontsize=12, pad=10)
        ax3.set_ylabel('Time (seconds)')

        # Add Monaco circuit sector information
        fig.text(0.02, 0.98, 'Monaco Circuit Sectors:', fontsize=10, fontweight='bold')
        fig.text(0.02, 0.96, 'S1: Casino Square to Mirabeau', fontsize=8)
        fig.text(0.02, 0.94, 'S2: Hairpin to Portier Tunnel', fontsize=8)
        fig.text(0.02, 0.92, 'S3: Tunnel to Swimming Pool Complex', fontsize=8)

        plt.tight_layout()
        plt.show()

        # Print additional sector analysis
        print("\nDetailed Sector Analysis:")
        print("-" * 50)
        for col in sector_cols:
            print(f"{col}:")
            print(f"  Mean: {X[col].mean():.3f}s")
            print(f"  Std Dev: {X[col].std():.3f}s")
            print(f"  Min: {X[col].min():.3f}s")
            print(f"  Max: {X[col].max():.3f}s")

    def _plot_qualifying_impact(self, X):
        """Plot qualifying position impact on race performance"""
        plt.figure(figsize=(10, 6))

        # Check if we have qualifying-related columns
        quali_cols = [col for col in X.columns if 'Quali' in col or 'Position' in col]
        if not quali_cols:
            print("Warning: No qualifying-related columns found for visualization")
            plt.close()
            return

        # Select the most relevant column for x-axis
        x_col = None
        preferred_cols = ['QualiPosition', 'QualifyingPosition', 'Position', 'AvgPosition']
        for col in preferred_cols:
            if col in X.columns:
                x_col = col
                break

        if x_col is None:
            print("Warning: No suitable position column found for visualization")
            plt.close()
            return

        # Plot the relationship
        sns.scatterplot(data=X, x=x_col, y='LapTime_seconds' if 'LapTime_seconds' in X.columns else x_col)
        plt.title(f'{x_col} vs Performance at Monaco')
        plt.xlabel(x_col)
        plt.ylabel('Lap Time (seconds)' if 'LapTime_seconds' in X.columns else x_col)
        plt.tight_layout()
        plt.show()

def main():
    # Initialize predictor
    predictor = MonacoGPPredictor()

    # Load and preprocess data
    print("\nLoading historical data...")
    race_data = predictor.load_historical_data()
    if race_data is None:
        print("Error: Could not load historical data")
        return

    print(f"\nInitial data shape: {race_data.shape}")
    print("Initial columns:", race_data.columns.tolist())

    # Fetch weather data with provided API key
    weather_data = predictor.fetch_monaco_weather("4e078bd52791a7bbc6b52a90051e12f7")
    if weather_data is None:
        print("Warning: Could not fetch weather data. Proceeding without weather features.")
    else:
        print("\nWeather data loaded successfully:")
        print(f"Air Temperature: {weather_data['air_temp']}°C")
        print(f"Track Temperature: {weather_data['track_temp']}°C")
        print(f"Humidity: {weather_data['humidity']*100}%")
        print(f"Wind Speed: {weather_data['wind_speed']} m/s")
        print(f"Wind Direction: {weather_data['wind_direction']}°")

    # Preprocess data
    print("\nPreprocessing data...")
    processed_data = predictor.preprocess_monaco_data(race_data, weather_data)
    if processed_data is None:
        print("Error: Data preprocessing failed")
        return

    print(f"\nProcessed data shape: {processed_data.shape}")

    # Check for LapTime_seconds column
    if 'LapTime_seconds' not in processed_data.columns:
        print("\nError: Target variable 'LapTime_seconds' not found in processed data")
        print("Available columns:", processed_data.columns.tolist())
        return

    # Print value counts for non-null LapTime_seconds
    print("\nNumber of valid lap times:", processed_data['LapTime_seconds'].notna().sum())
    print("Number of null lap times:", processed_data['LapTime_seconds'].isna().sum())

    # Split features and target
    y = processed_data['LapTime_seconds']
    X = processed_data.drop(['LapTime_seconds', 'Driver', 'Team'], axis=1, errors='ignore')

    # Print initial feature stats
    print(f"\nInitial feature set shape: {X.shape}")

    # Ensure all features are numeric
    numeric_X = X.select_dtypes(include=[np.number])
    print(f"\nNumeric feature set shape: {numeric_X.shape}")

    if len(numeric_X.columns) == 0:
        print("Error: No numeric features available for training")
        return

    # Drop problematic columns that are not needed for prediction
    columns_to_drop = ['DeletedReason']
    numeric_X = numeric_X.drop(columns=columns_to_drop, errors='ignore')
    print("\nDropped unnecessary columns:", columns_to_drop)

    # Check for NaN values before cleaning
    nan_cols = numeric_X.columns[numeric_X.isnull().any()].tolist()
    if nan_cols:
        print("\nColumns with NaN values before cleaning:", nan_cols)
        print("NaN counts per column:")
        for col in nan_cols:
            print(f"{col}: {numeric_X[col].isnull().sum()} NaN values")

    # Remove rows with NaN values
    mask = ~numeric_X.isnull().any(axis=1)
    numeric_X = numeric_X[mask]
    y = y[mask]

    print(f"\nFinal cleaned data shape: {numeric_X.shape}")

    if len(numeric_X) < 10:  # Arbitrary minimum threshold
        print("Error: Insufficient data points after cleaning")
        return

    print("\nFeatures used in training:", numeric_X.columns.tolist())
    print("Number of features:", len(numeric_X.columns))
    print("Number of samples:", len(numeric_X))

    # Adjust number of splits based on data size
    n_splits = min(5, len(numeric_X) // 2)  # Ensure we have enough samples per split
    if n_splits < 2:
        print("Error: Not enough samples for time series cross-validation")
        return

    print(f"\nUsing {n_splits} splits for time series cross-validation")

    # Split data
    tscv = TimeSeriesSplit(n_splits=n_splits)
    for fold, (train_idx, val_idx) in enumerate(tscv.split(numeric_X), 1):
        print(f"\nProcessing fold {fold}/{n_splits}")
        print(f"Train set size: {len(train_idx)}, Validation set size: {len(val_idx)}")

        X_train, X_val = numeric_X.iloc[train_idx], numeric_X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        try:
            # Train models
            predictor.train_monaco_models(X_train, y_train, X_val, y_val)

            # Make predictions
            predictions = predictor.predict_monaco_performance(X_val)

            # Verify predictions shape
            if len(predictions) != len(y_val):
                print(f"Error: Prediction shape mismatch. Expected {len(y_val)}, got {len(predictions)}")
                continue

            # Create a DataFrame with predictions and driver information
            results_df = pd.DataFrame({
                'Driver': processed_data.iloc[val_idx]['Driver'] if 'Driver' in processed_data.columns else 'Unknown',
                'Team': processed_data.iloc[val_idx]['Team'] if 'Team' in processed_data.columns else 'Unknown',
                'PredictedLapTime': predictions,
                'ActualLapTime': y_val
            })

            # Sort by predicted lap time and get unique drivers (best lap per driver)
            top_drivers = (
                results_df.sort_values('PredictedLapTime')
                .drop_duplicates('Driver')
                .head(10)
            )

            print("\nPredicted Top 10 at Monaco:")
            print("-" * 50)
            for i, (_, row) in enumerate(top_drivers.iterrows(), 1):
                driver = row['Driver']
                team = row['Team']
                pred_time = row['PredictedLapTime']
                actual_time = row['ActualLapTime']
                delta = pred_time - actual_time
                print(f"P{i}: {driver} ({team}) - {pred_time:.3f}s (Δ {delta:+.3f}s)")
            print("-" * 50)

            # Analyze performance
            try:
                predictor.analyze_monaco_performance(X_val)
            except Exception as e:
                print(f"Warning: Could not complete performance analysis: {e}")

            # Calculate and print metrics
            print("\nMonaco GP Model Performance Metrics:")
            print(f"MAE: {mean_absolute_error(y_val, predictions):.3f}")
            print(f"R2 Score: {r2_score(y_val, predictions):.3f}")

        except Exception as e:
            print(f"Error in training/prediction loop: {e}")
            continue

if __name__ == "__main__":
    main()