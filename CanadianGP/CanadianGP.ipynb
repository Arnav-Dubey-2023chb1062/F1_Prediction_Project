{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJlsxwdpto8Y",
        "outputId": "3360236b-82c6-4ff7-c44e-3070835b890a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastf1\n",
            "  Downloading fastf1-3.5.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.6)\n",
            "Collecting feature-engine\n",
            "  Downloading feature_engine-1.8.3-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting mlFlow\n",
            "  Downloading mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from fastf1) (2.8.2)\n",
            "Collecting rapidfuzz (from fastf1)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting requests-cache>=1.0.0 (from fastf1)\n",
            "  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from fastf1) (1.13.1)\n",
            "Collecting timple>=0.1.6 (from fastf1)\n",
            "  Downloading timple-0.1.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting websockets<14,>=10.3 (from fastf1)\n",
            "  Downloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.68)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from feature-engine) (0.14.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Collecting mlflow-skinny==2.22.0 (from mlFlow)\n",
            "  Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlFlow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlFlow) (3.1.5)\n",
            "Collecting docker<8,>=4.0.0 (from mlFlow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlFlow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlFlow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlFlow) (3.7)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlFlow) (18.1.0)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlFlow)\n",
            "  Downloading databricks_sdk-0.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting fastapi<1 (from mlflow-skinny==2.22.0->mlFlow)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (1.16.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (4.25.6)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (2.10.6)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlFlow) (4.12.2)\n",
            "Collecting uvicorn<1 (from mlflow-skinny==2.22.0->mlFlow)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlFlow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlFlow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlFlow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlFlow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlFlow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlFlow) (3.0.2)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.11/dist-packages (from requests-cache>=1.0.0->fastf1) (25.1.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache>=1.0.0->fastf1)\n",
            "  Downloading cattrs-25.1.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-cache>=1.0.0->fastf1) (4.3.6)\n",
            "Collecting url-normalize>=1.4 (from requests-cache>=1.0.0->fastf1)\n",
            "  Downloading url_normalize-2.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11.1->feature-engine) (1.0.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlFlow) (2.38.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.0->mlFlow)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlFlow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlFlow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlFlow) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlFlow) (75.1.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlFlow) (0.37b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlFlow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlFlow) (2.27.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==2.22.0->mlFlow) (0.14.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlFlow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlFlow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlFlow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlFlow) (4.9)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlFlow) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlFlow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlFlow) (0.6.1)\n",
            "Downloading fastf1-3.5.3-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feature_engine-1.8.3-py2.py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.6/378.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timple-0.1.8-py3-none-any.whl (17 kB)\n",
            "Downloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cattrs-25.1.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.55.0-py3-none-any.whl (722 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.9/722.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading url_normalize-2.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, uvicorn, url-normalize, rapidfuzz, Mako, gunicorn, graphql-core, colorlog, cattrs, starlette, requests-cache, graphql-relay, docker, alembic, timple, optuna, graphene, fastapi, databricks-sdk, catboost, mlflow-skinny, feature-engine, fastf1, mlFlow\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "Successfully installed Mako-1.3.10 alembic-1.16.1 catboost-1.2.8 cattrs-25.1.0 colorlog-6.9.0 databricks-sdk-0.55.0 docker-7.1.0 fastapi-0.115.12 fastf1-3.5.3 feature-engine-1.8.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlFlow-2.22.0 mlflow-skinny-2.22.0 optuna-4.3.0 rapidfuzz-3.13.0 requests-cache-1.2.1 starlette-0.46.2 timple-0.1.8 url-normalize-2.2.1 uvicorn-0.34.2 websockets-13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fastf1 pandas numpy scikit-learn matplotlib seaborn requests optuna lightgbm prophet feature-engine catboost mlFlow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fastf1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "from prophet import Prophet\n",
        "from tensorflow import keras\n",
        "import mlflow\n",
        "import shap\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.creation import CyclicalFeatures\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# MLflow setup\n",
        "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
        "mlflow.set_experiment(\"F1_Canada_Predictions\")\n",
        "\n",
        "class WeatherAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.weather_impact = {\n",
        "            'DRY': 1.0,\n",
        "            'LIGHT_RAIN': 1.05,\n",
        "            'RAIN': 1.15,\n",
        "            'HEAVY_RAIN': 1.25\n",
        "        }\n",
        "        self.track_temp_optimal = 35  # Optimal track temperature for Montreal\n",
        "\n",
        "    def calculate_weather_impact(self, conditions, track_temp, air_temp):\n",
        "        \"\"\"Calculate weather impact on lap times\"\"\"\n",
        "        base_impact = self.weather_impact.get(conditions, 1.0)\n",
        "\n",
        "        # Temperature impact (Montreal specific)\n",
        "        temp_delta = abs(track_temp - self.track_temp_optimal)\n",
        "        temp_impact = 1 + (temp_delta * 0.002)  # 0.2% per degree difference\n",
        "\n",
        "        # Air density impact (important for Montreal's long straights)\n",
        "        air_density_impact = 1 + ((15 - air_temp) * 0.001)\n",
        "\n",
        "        # Humidity impact (Montreal can be humid in June)\n",
        "        humidity_impact = 1 + (0.001 * (air_temp - 15))  # More impact at higher temperatures\n",
        "\n",
        "        return base_impact * temp_impact * air_density_impact * humidity_impact\n",
        "\n"
      ],
      "metadata": {
        "id": "eRP07QlZuT5F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TireStrategyAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.compound_characteristics = {\n",
        "            'SOFT': {'peak_grip': 1.0, 'wear_rate': 0.12, 'optimal_temp': 90, 'durability': 0.8},\n",
        "            'MEDIUM': {'peak_grip': 0.97, 'wear_rate': 0.08, 'optimal_temp': 85, 'durability': 1.0},\n",
        "            'HARD': {'peak_grip': 0.94, 'wear_rate': 0.05, 'optimal_temp': 80, 'durability': 1.2},\n",
        "            'INTERMEDIATE': {'peak_grip': 0.96, 'wear_rate': 0.10, 'optimal_temp': 75, 'durability': 0.9},\n",
        "            'WET': {'peak_grip': 0.93, 'wear_rate': 0.07, 'optimal_temp': 65, 'durability': 1.1}\n",
        "        }\n",
        "\n",
        "    def calculate_tire_performance(self, compound, lap_number, track_temp):\n",
        "        \"\"\"Calculate tire performance considering Montreal's characteristics\"\"\"\n",
        "        # Handle unknown compounds\n",
        "        if compound not in self.compound_characteristics:\n",
        "            print(f\"Warning: Unknown tire compound '{compound}', using MEDIUM characteristics\")\n",
        "            char = self.compound_characteristics['MEDIUM']\n",
        "        else:\n",
        "            char = self.compound_characteristics[compound]\n",
        "\n",
        "        # Base performance\n",
        "        base_perf = char['peak_grip']\n",
        "\n",
        "        # Wear effect (Montreal is medium on tires)\n",
        "        wear = lap_number * char['wear_rate']\n",
        "\n",
        "        # Temperature effect\n",
        "        temp_delta = abs(track_temp - char['optimal_temp'])\n",
        "        temp_effect = 1 - (temp_delta * 0.002)\n",
        "\n",
        "        # Montreal-specific adjustments\n",
        "        hairpin_impact = 1 - (wear * 1.2)  # Heavy braking zones\n",
        "        straight_impact = 1 - (wear * 0.8)  # Long straights\n",
        "\n",
        "        # Durability factor\n",
        "        durability_factor = 1 - (wear * (1 - char['durability']))\n",
        "\n",
        "        return base_perf * (1 - wear) * temp_effect * hairpin_impact * straight_impact * durability_factor\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        \"\"\"Preprocess data with Canadian GP specific handling\"\"\"\n",
        "        try:\n",
        "            processed_data = data.copy()\n",
        "\n",
        "            # Handle time-based columns\n",
        "            time_cols = [col for col in processed_data.columns\n",
        "                        if any(t in col.lower() for t in ['time', 'lap', 'sector'])]\n",
        "\n",
        "            for col in time_cols:\n",
        "                if pd.api.types.is_timedelta64_dtype(processed_data[col]):\n",
        "                    processed_data[f'{col}_seconds'] = processed_data[col].dt.total_seconds()\n",
        "                    processed_data = processed_data.drop(col, axis=1)\n",
        "\n",
        "            # Replace empty strings and other problematic values with NaN\n",
        "            processed_data = processed_data.replace(['', 'None', 'NaN', 'nan', 'NULL'], np.nan)\n",
        "\n",
        "            # Handle datetime columns\n",
        "            if 'LapStartDate' in processed_data.columns:\n",
        "                try:\n",
        "                    # Convert to datetime if not already\n",
        "                    processed_data['LapStartDate'] = pd.to_datetime(processed_data['LapStartDate'])\n",
        "                    # Extract useful features from datetime\n",
        "                    processed_data['DayOfYear'] = processed_data['LapStartDate'].dt.dayofyear\n",
        "                    processed_data['MonthOfYear'] = processed_data['LapStartDate'].dt.month\n",
        "                    processed_data['DayOfWeek'] = processed_data['LapStartDate'].dt.dayofweek\n",
        "                    # Drop original column\n",
        "                    processed_data = processed_data.drop('LapStartDate', axis=1)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Error processing LapStartDate: {e}\")\n",
        "                    processed_data = processed_data.drop('LapStartDate', axis=1)\n",
        "\n",
        "            # Handle categorical variables\n",
        "            cat_cols = ['Driver', 'Team', 'Compound', 'TrackStatus', 'Session']\n",
        "            for col in cat_cols:\n",
        "                if col in processed_data.columns:\n",
        "                    if col == 'Compound':\n",
        "                        # Create dummy variables for tire compounds\n",
        "                        compound_dummies = pd.get_dummies(processed_data[col], prefix='Compound')\n",
        "                        processed_data = pd.concat([processed_data, compound_dummies], axis=1)\n",
        "                        processed_data = processed_data.drop('Compound', axis=1)  # Drop original column\n",
        "                    else:\n",
        "                        # Fill NaN with a placeholder before encoding\n",
        "                        processed_data[col] = processed_data[col].fillna('Unknown')\n",
        "                        # Create both label encoding and one-hot encoding\n",
        "                        processed_data[f'{col}_encoded'] = pd.Categorical(processed_data[col]).codes\n",
        "                        # Create one-hot encoding for the categorical columns\n",
        "                        dummies = pd.get_dummies(processed_data[col], prefix=col)\n",
        "                        processed_data = pd.concat([processed_data, dummies], axis=1)\n",
        "                        processed_data = processed_data.drop(col, axis=1)\n",
        "\n",
        "            # Create session-specific features\n",
        "            processed_data['IsRace'] = (processed_data['Session'] == 'Race').astype(int)\n",
        "            processed_data['IsQuali'] = (processed_data['Session'] == 'Qualifying').astype(int)\n",
        "            processed_data['IsSprint'] = (processed_data['Session'] == 'Sprint').astype(int)\n",
        "\n",
        "            # Convert specific columns to numeric, handling errors\n",
        "            numeric_cols = [\n",
        "                'LapNumber', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST',\n",
        "                'TyreLife', 'Position', 'Year', 'DayOfYear', 'MonthOfYear', 'DayOfWeek'\n",
        "            ]\n",
        "\n",
        "            for col in numeric_cols:\n",
        "                if col in processed_data.columns:\n",
        "                    try:\n",
        "                        processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error converting {col} to numeric: {e}\")\n",
        "\n",
        "            # Handle missing values for numeric columns\n",
        "            numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_cols:\n",
        "                if processed_data[col].isnull().any():\n",
        "                    if 'Time' in col or 'time' in col:\n",
        "                        # For time columns, use median to avoid outlier effects\n",
        "                        median_val = processed_data[col].median()\n",
        "                        processed_data[col] = processed_data[col].fillna(median_val)\n",
        "                    elif col in ['Position', 'LapNumber', 'Stint']:\n",
        "                        # For these columns, use forward fill then backward fill\n",
        "                        processed_data[col] = processed_data[col].ffill().bfill()\n",
        "                    elif col in ['Year']:\n",
        "                        # For year, use mode\n",
        "                        mode_val = processed_data[col].mode()[0]\n",
        "                        processed_data[col] = processed_data[col].fillna(mode_val)\n",
        "                    else:\n",
        "                        # For other numeric columns, use mean\n",
        "                        mean_val = processed_data[col].mean()\n",
        "                        processed_data[col] = processed_data[col].fillna(mean_val)\n",
        "\n",
        "            # Drop non-essential columns that might cause issues\n",
        "            cols_to_drop = ['IsPersonalBest', 'DeletedReason', 'Deleted', 'FastF1Generated', 'IsAccurate', 'DriverNumber', 'FreshTyre']\n",
        "            processed_data = processed_data.drop([col for col in cols_to_drop if col in processed_data.columns], axis=1)\n",
        "\n",
        "            # Ensure all remaining numeric columns are float64\n",
        "            numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_cols:\n",
        "                processed_data[col] = processed_data[col].astype(np.float64)\n",
        "\n",
        "            # Print debugging information\n",
        "            print(\"\\nProcessed data info:\")\n",
        "            print(processed_data.info())\n",
        "            print(\"\\nNumeric columns:\", numeric_cols.tolist())\n",
        "            print(\"\\nMissing values after processing:\")\n",
        "            print(processed_data.isnull().sum()[processed_data.isnull().sum() > 0])\n",
        "\n",
        "            # Final check for any remaining non-numeric data in numeric columns\n",
        "            for col in numeric_cols:\n",
        "                non_numeric = processed_data[pd.to_numeric(processed_data[col], errors='coerce').isnull()]\n",
        "                if len(non_numeric) > 0:\n",
        "                    print(f\"\\nWarning: Found non-numeric values in {col}:\")\n",
        "                    print(non_numeric[col].unique())\n",
        "\n",
        "            return processed_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocessing: {e}\")\n",
        "            print(\"Data columns:\", data.columns.tolist())\n",
        "            if 'Compound' in data.columns:\n",
        "                print(\"\\nUnique compounds:\", data['Compound'].unique())\n",
        "            return None\n",
        "\n",
        "    def _add_montreal_features(self, data):\n",
        "        \"\"\"Add Montreal-specific features to the dataset\"\"\"\n",
        "        try:\n",
        "            # Weather impact\n",
        "            if 'WeatherCondition' in data.columns:\n",
        "                data['WeatherImpact'] = data.apply(\n",
        "                    lambda x: self.weather_analyzer.calculate_weather_impact(\n",
        "                        x['WeatherCondition'],\n",
        "                        x.get('TrackTemp', 25),\n",
        "                        x.get('AirTemp', 20)\n",
        "                    ), axis=1\n",
        "                )\n",
        "\n",
        "            # Tire performance\n",
        "            if 'Compound' in data.columns:\n",
        "                # Print unique compounds for debugging\n",
        "                print(\"\\nUnique tire compounds found:\", data['Compound'].unique())\n",
        "\n",
        "                data['TirePerformance'] = data.apply(\n",
        "                    lambda x: self.tire_analyzer.calculate_tire_performance(\n",
        "                        x['Compound'],\n",
        "                        x['LapNumber'],\n",
        "                        x.get('TrackTemp', 25)\n",
        "                    ), axis=1\n",
        "                )\n",
        "\n",
        "            # DRS effectiveness (Montreal has long DRS zones)\n",
        "            if 'DRS' in data.columns:\n",
        "                data['DRSEffect'] = data['DRS'] * 0.3  # 30% performance gain potential\n",
        "\n",
        "            # Track evolution\n",
        "            data['TrackEvolution'] = data.groupby('Year')['LapNumber'].transform(\n",
        "                lambda x: (x / x.max()) * 0.1 + 1  # Up to 10% improvement\n",
        "            )\n",
        "\n",
        "            # Brake wear impact (Montreal is hard on brakes)\n",
        "            data['BrakeWear'] = data.apply(\n",
        "                lambda x: 1 - (x['LapNumber'] / 70) * 0.15  # Up to 15% degradation\n",
        "                if x['Session'] == 'Race' else 1.0,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding Montreal features: {e}\")\n",
        "            print(\"Data columns:\", data.columns.tolist())\n",
        "            print(\"\\nSample of problematic data:\")\n",
        "            if 'Compound' in data.columns:\n",
        "                print(data[['Compound', 'LapNumber', 'TrackTemp']].head())\n",
        "            return data\n",
        "\n"
      ],
      "metadata": {
        "id": "I1pAatW196-F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CanadianGPPredictor:\n",
        "    def __init__(self):\n",
        "        self.setup_cache()\n",
        "        self.weather_analyzer = WeatherAnalyzer()\n",
        "        self.tire_analyzer = TireStrategyAnalyzer()\n",
        "        self.scalers = {\n",
        "            'standard': StandardScaler(),\n",
        "            'robust': RobustScaler()\n",
        "        }\n",
        "        self.models = {\n",
        "            'lightgbm': None,\n",
        "            'xgboost': None,\n",
        "            'catboost': None,\n",
        "            'neural_net': None\n",
        "        }\n",
        "\n",
        "        # 2025 F1 Driver Lineup\n",
        "        self.drivers_2025 = {\n",
        "            'VER': {'name': 'Max Verstappen', 'team': 'Red Bull Racing', 'performance_factor': 0.995},\n",
        "            'NOR': {'name': 'Lando Norris', 'team': 'McLaren', 'performance_factor': 0.996},\n",
        "            'PIA': {'name': 'Oscar Piastri', 'team': 'McLaren', 'performance_factor': 0.997},\n",
        "            'RUS': {'name': 'George Russell', 'team': 'Mercedes', 'performance_factor': 0.997},\n",
        "            'SAI': {'name': 'Carlos Sainz', 'team': 'Audi', 'performance_factor': 0.998},\n",
        "            'ALB': {'name': 'Alexander Albon', 'team': 'Williams', 'performance_factor': 1.000},\n",
        "            'LEC': {'name': 'Charles Leclerc', 'team': 'Ferrari', 'performance_factor': 0.996},\n",
        "            'OCO': {'name': 'Esteban Ocon', 'team': 'Alpine', 'performance_factor': 1.000},\n",
        "            'HAM': {'name': 'Lewis Hamilton', 'team': 'Ferrari', 'performance_factor': 0.997},\n",
        "            'STR': {'name': 'Lance Stroll', 'team': 'Aston Martin', 'performance_factor': 1.001},\n",
        "            'GAS': {'name': 'Pierre Gasly', 'team': 'Alpine', 'performance_factor': 1.000},\n",
        "            'ALO': {'name': 'Fernando Alonso', 'team': 'Aston Martin', 'performance_factor': 0.998},\n",
        "            'HUL': {'name': 'Nico Hulkenberg', 'team': 'Haas', 'performance_factor': 1.001}\n",
        "        }\n",
        "\n",
        "        # Team performance factors for 2025\n",
        "        self.team_factors = {\n",
        "            'Red Bull Racing': 0.995,  # Dominant team\n",
        "            'McLaren': 0.997,         # Continued improvement\n",
        "            'Mercedes': 0.998,        # Recovery\n",
        "            'Audi': 0.999,           # New team but strong resources\n",
        "            'Williams': 1.001,        # Improving\n",
        "            'Ferrari': 0.997,         # Strong development\n",
        "            'Alpine': 1.000,         # Midfield\n",
        "            'Aston Martin': 0.999,    # Stable\n",
        "            'Haas': 1.002            # Backmarker\n",
        "        }\n",
        "\n",
        "        # Montreal-specific constants\n",
        "        self.TRACK_SECTORS = {\n",
        "            'S1': ['Turn1', 'Turn2'],\n",
        "            'S2': ['Hairpin', 'Back_Straight'],\n",
        "            'S3': ['Wall_Champions', 'Final_Chicane']\n",
        "        }\n",
        "        self.SPRINT_IMPORTANCE = 0.3\n",
        "        self.QUALI_IMPORTANCE = 0.4\n",
        "\n",
        "    def setup_cache(self):\n",
        "        \"\"\"Setup FastF1 cache\"\"\"\n",
        "        os.makedirs(\"f1_cache\", exist_ok=True)\n",
        "        fastf1.Cache.enable_cache(\"f1_cache\")\n",
        "\n",
        "    def load_historical_data(self):\n",
        "        \"\"\"Load historical Canadian GP data including sprint races\"\"\"\n",
        "        try:\n",
        "            all_data = []\n",
        "\n",
        "            # Load race, qualifying, and sprint data for recent years\n",
        "            for year in [2024, 2023, 2022]:\n",
        "                print(f\"\\nLoading {year} data...\")\n",
        "\n",
        "                # Load race data\n",
        "                race = fastf1.get_session(year, \"Canada\", \"R\")\n",
        "                race.load()\n",
        "                race_data = race.laps.copy()\n",
        "                race_data['Session'] = 'Race'\n",
        "                race_data['Year'] = year\n",
        "\n",
        "                # Load qualifying data\n",
        "                quali = fastf1.get_session(year, \"Canada\", \"Q\")\n",
        "                quali.load()\n",
        "                quali_data = quali.laps.copy()\n",
        "                quali_data['Session'] = 'Qualifying'\n",
        "                quali_data['Year'] = year\n",
        "\n",
        "                # Load sprint data if available\n",
        "                try:\n",
        "                    sprint = fastf1.get_session(year, \"Canada\", \"S\")\n",
        "                    sprint.load()\n",
        "                    sprint_data = sprint.laps.copy()\n",
        "                    sprint_data['Session'] = 'Sprint'\n",
        "                    sprint_data['Year'] = year\n",
        "                    all_data.append(sprint_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"No sprint data for {year}: {e}\")\n",
        "\n",
        "                all_data.extend([race_data, quali_data])\n",
        "\n",
        "            # Combine all data\n",
        "            combined_data = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "            # Add Montreal-specific features\n",
        "            combined_data = self._add_montreal_features(combined_data)\n",
        "\n",
        "            return combined_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Canadian GP data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        \"\"\"Preprocess data with Canadian GP specific handling\"\"\"\n",
        "        try:\n",
        "            processed_data = data.copy()\n",
        "            print(\"\\nStarting data preprocessing...\")\n",
        "            print(\"Initial columns:\", processed_data.columns.tolist())\n",
        "\n",
        "            # Replace empty strings and other problematic values with NaN\n",
        "            processed_data = processed_data.replace(['', 'None', 'NaN', 'nan', 'NULL'], np.nan)\n",
        "\n",
        "            # Create session-specific features first (before dropping Session column)\n",
        "            processed_data['IsRace'] = (processed_data['Session'] == 'Race').astype(int)\n",
        "            processed_data['IsQuali'] = (processed_data['Session'] == 'Qualifying').astype(int)\n",
        "            processed_data['IsSprint'] = (processed_data['Session'] == 'Sprint').astype(int)\n",
        "\n",
        "            # Handle time-based columns\n",
        "            time_cols = [col for col in processed_data.columns\n",
        "                        if any(t in col.lower() for t in ['time', 'lap', 'sector'])]\n",
        "\n",
        "            for col in time_cols:\n",
        "                if pd.api.types.is_timedelta64_dtype(processed_data[col]):\n",
        "                    processed_data[f'{col}_seconds'] = processed_data[col].dt.total_seconds()\n",
        "                    processed_data = processed_data.drop(col, axis=1)\n",
        "\n",
        "            # Handle datetime columns\n",
        "            if 'LapStartDate' in processed_data.columns:\n",
        "                try:\n",
        "                    processed_data['LapStartDate'] = pd.to_datetime(processed_data['LapStartDate'])\n",
        "                    processed_data['DayOfYear'] = processed_data['LapStartDate'].dt.dayofyear\n",
        "                    processed_data['MonthOfYear'] = processed_data['LapStartDate'].dt.month\n",
        "                    processed_data['DayOfWeek'] = processed_data['LapStartDate'].dt.dayofweek\n",
        "                    processed_data = processed_data.drop('LapStartDate', axis=1)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Error processing LapStartDate: {e}\")\n",
        "                    processed_data = processed_data.drop('LapStartDate', axis=1)\n",
        "\n",
        "            # Handle categorical variables with one-hot encoding\n",
        "            cat_cols = ['Driver', 'Team', 'Compound', 'TrackStatus', 'Session', 'WeatherCondition']\n",
        "            for col in cat_cols:\n",
        "                if col in processed_data.columns:\n",
        "                    print(f\"\\nProcessing categorical column: {col}\")\n",
        "                    # Ensure the column contains single values, not repeated strings\n",
        "                    if col == 'WeatherCondition':\n",
        "                        processed_data[col] = processed_data[col].str[:4]  # Take first 4 characters if repeated\n",
        "                    print(f\"Unique values in {col}:\", processed_data[col].unique())\n",
        "\n",
        "                    # Fill NaN values\n",
        "                    processed_data[col] = processed_data[col].fillna('Unknown')\n",
        "\n",
        "                    # Create one-hot encoding\n",
        "                    try:\n",
        "                        dummies = pd.get_dummies(processed_data[col], prefix=col)\n",
        "                        processed_data = pd.concat([processed_data, dummies], axis=1)\n",
        "                        processed_data = processed_data.drop(col, axis=1)\n",
        "                        print(f\"Successfully created dummies for {col}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error creating dummies for {col}: {e}\")\n",
        "                        return None\n",
        "\n",
        "            # Convert specific columns to numeric\n",
        "            numeric_cols = [\n",
        "                'LapNumber', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST',\n",
        "                'TyreLife', 'Position', 'Year', 'DayOfYear', 'MonthOfYear', 'DayOfWeek',\n",
        "                'TirePerformance', 'TrackEvolution', 'BrakeWear', 'LapTime_seconds',\n",
        "                'TrackTemp', 'AirTemp', 'Humidity', 'WindSpeed', 'WindDirection', 'CloudCover'\n",
        "            ]\n",
        "\n",
        "            for col in numeric_cols:\n",
        "                if col in processed_data.columns:\n",
        "                    try:\n",
        "                        processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error converting {col} to numeric: {e}\")\n",
        "\n",
        "            # Handle missing values for numeric columns\n",
        "            numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_cols:\n",
        "                if processed_data[col].isnull().any():\n",
        "                    if 'Time' in col or 'time' in col:\n",
        "                        median_val = processed_data[col].median()\n",
        "                        processed_data[col] = processed_data[col].fillna(median_val)\n",
        "                    elif col in ['Position', 'LapNumber', 'Stint']:\n",
        "                        processed_data[col] = processed_data[col].ffill().bfill()\n",
        "                    elif col in ['Year']:\n",
        "                        mode_val = processed_data[col].mode()[0]\n",
        "                        processed_data[col] = processed_data[col].fillna(mode_val)\n",
        "                    else:\n",
        "                        mean_val = processed_data[col].mean()\n",
        "                        processed_data[col] = processed_data[col].fillna(mean_val)\n",
        "\n",
        "            # Drop non-essential columns\n",
        "            cols_to_drop = [\n",
        "                'IsPersonalBest', 'DeletedReason', 'Deleted', 'FastF1Generated',\n",
        "                'IsAccurate', 'DriverNumber', 'FreshTyre', 'Time'\n",
        "            ]\n",
        "            processed_data = processed_data.drop([col for col in cols_to_drop if col in processed_data.columns], axis=1)\n",
        "\n",
        "            # Ensure all remaining numeric columns are float64\n",
        "            numeric_cols = processed_data.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_cols:\n",
        "                processed_data[col] = processed_data[col].astype(np.float64)\n",
        "\n",
        "            # Ensure no infinite values\n",
        "            processed_data = processed_data.replace([np.inf, -np.inf], np.nan)\n",
        "            processed_data = processed_data.fillna(processed_data.mean())\n",
        "\n",
        "            print(\"\\nFinal columns after preprocessing:\", processed_data.columns.tolist())\n",
        "            print(\"\\nNumeric columns:\", numeric_cols.tolist())\n",
        "            print(\"\\nMissing values after processing:\")\n",
        "            print(processed_data.isnull().sum()[processed_data.isnull().sum() > 0])\n",
        "\n",
        "            return processed_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in preprocessing: {e}\")\n",
        "            print(\"Data columns:\", data.columns.tolist())\n",
        "            if 'Compound' in data.columns:\n",
        "                print(\"\\nUnique compounds:\", data['Compound'].unique())\n",
        "            return None\n",
        "\n",
        "    def _add_montreal_features(self, data):\n",
        "        \"\"\"Add Montreal-specific features to the dataset\"\"\"\n",
        "        try:\n",
        "            # Weather impact\n",
        "            if 'WeatherCondition' in data.columns:\n",
        "                data['WeatherImpact'] = data.apply(\n",
        "                    lambda x: self.weather_analyzer.calculate_weather_impact(\n",
        "                        x['WeatherCondition'],\n",
        "                        x.get('TrackTemp', 25),\n",
        "                        x.get('AirTemp', 20)\n",
        "                    ), axis=1\n",
        "                )\n",
        "\n",
        "            # Tire performance\n",
        "            if 'Compound' in data.columns:\n",
        "                print(\"\\nUnique tire compounds found:\", data['Compound'].unique())\n",
        "                data['TirePerformance'] = data.apply(\n",
        "                    lambda x: self.tire_analyzer.calculate_tire_performance(\n",
        "                        x['Compound'],\n",
        "                        x['LapNumber'],\n",
        "                        x.get('TrackTemp', 25)\n",
        "                    ), axis=1\n",
        "                )\n",
        "\n",
        "            # DRS effectiveness (Montreal has long DRS zones)\n",
        "            if 'DRS' in data.columns:\n",
        "                data['DRSEffect'] = data['DRS'] * 0.3  # 30% performance gain potential\n",
        "\n",
        "            # Track evolution\n",
        "            data['TrackEvolution'] = data.groupby('Year')['LapNumber'].transform(\n",
        "                lambda x: (x / x.max()) * 0.1 + 1  # Up to 10% improvement\n",
        "            )\n",
        "\n",
        "            # Brake wear impact (Montreal is hard on brakes)\n",
        "            data['BrakeWear'] = data.apply(\n",
        "                lambda x: 1 - (x['LapNumber'] / 70) * 0.15  # Up to 15% degradation\n",
        "                if x['Session'] == 'Race' else 1.0,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding Montreal features: {e}\")\n",
        "            print(\"Data columns:\", data.columns.tolist())\n",
        "            print(\"\\nSample of problematic data:\")\n",
        "            if 'Compound' in data.columns:\n",
        "                print(data[['Compound', 'LapNumber', 'TrackTemp']].head())\n",
        "            return data\n",
        "\n",
        "    def create_canadian_neural_network(self, input_shape):\n",
        "        \"\"\"Create a neural network optimized for Canadian GP predictions\"\"\"\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(256, activation='relu', input_shape=input_shape),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.4),\n",
        "            keras.layers.Dense(128, activation='relu'),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.3),\n",
        "            keras.layers.Dense(64, activation='relu'),\n",
        "            keras.layers.Dense(32, activation='relu'),\n",
        "            keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='huber',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_models(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train multiple models with Canadian GP specific optimizations\"\"\"\n",
        "        with mlflow.start_run():\n",
        "            # Store training columns for prediction\n",
        "            self.training_columns = X_train.columns.tolist()\n",
        "\n",
        "            # Scale the data\n",
        "            self.X_scaler = StandardScaler()\n",
        "            self.y_scaler = StandardScaler()\n",
        "\n",
        "            # Ensure data is finite and handle any remaining issues\n",
        "            X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "            X_val = X_val.replace([np.inf, -np.inf], np.nan)\n",
        "            X_train = X_train.fillna(X_train.mean())\n",
        "            X_val = X_val.fillna(X_val.mean())\n",
        "\n",
        "            # Add interaction features\n",
        "            if 'SpeedI1' in X_train.columns and 'TrackGrip' in X_train.columns:\n",
        "                X_train['Speed_Track_Interaction'] = X_train['SpeedI1'] * X_train['TrackGrip']\n",
        "                X_val['Speed_Track_Interaction'] = X_val['SpeedI1'] * X_val['TrackGrip']\n",
        "\n",
        "            # Scale the data\n",
        "            X_train_scaled = self.X_scaler.fit_transform(X_train)\n",
        "            X_val_scaled = self.X_scaler.transform(X_val)\n",
        "            y_train_scaled = self.y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "            y_val_scaled = self.y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "            # Convert scaled arrays back to dataframes to preserve column names\n",
        "            X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "            X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_train.columns)\n",
        "\n",
        "            # Add polynomial features after scaling\n",
        "            if 'LapTime_seconds' in X_train_scaled.columns:\n",
        "                X_train_scaled['LapTime_seconds_squared'] = X_train_scaled['LapTime_seconds'] ** 2\n",
        "                X_train_scaled['LapTime_seconds_cubed'] = X_train_scaled['LapTime_seconds'] ** 3\n",
        "                X_val_scaled['LapTime_seconds_squared'] = X_val_scaled['LapTime_seconds'] ** 2\n",
        "                X_val_scaled['LapTime_seconds_cubed'] = X_val_scaled['LapTime_seconds'] ** 3\n",
        "\n",
        "            # Train LightGBM with updated parameters\n",
        "            self.models['lightgbm'] = lgb.LGBMRegressor(\n",
        "                objective='regression',\n",
        "                metric='rmse',\n",
        "                n_estimators=2000,  # Increased number of trees\n",
        "                learning_rate=0.005,  # Reduced learning rate\n",
        "                num_leaves=63,  # Increased number of leaves\n",
        "                min_child_samples=10,  # Reduced minimum samples per leaf\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                subsample_freq=5,\n",
        "                reg_alpha=0.01,  # Reduced L1 regularization\n",
        "                reg_lambda=0.01,  # Reduced L2 regularization\n",
        "                min_split_gain=0.0,  # Allow splits with minimal gain\n",
        "                min_child_weight=1,  # Reduced minimum child weight\n",
        "                random_state=42,\n",
        "                verbose=-1  # Suppress warnings\n",
        "            )\n",
        "\n",
        "            # Train XGBoost with updated parameters\n",
        "            self.models['xgboost'] = xgb.XGBRegressor(\n",
        "                objective='reg:squarederror',\n",
        "                n_estimators=2000,\n",
        "                learning_rate=0.005,\n",
        "                max_depth=8,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                min_child_weight=1,\n",
        "                gamma=0.01,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            # Train CatBoost with updated parameters\n",
        "            self.models['catboost'] = cb.CatBoostRegressor(\n",
        "                iterations=2000,\n",
        "                learning_rate=0.005,\n",
        "                depth=8,\n",
        "                loss_function='RMSE',\n",
        "                subsample=0.8,\n",
        "                rsm=0.8,\n",
        "                random_seed=42,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Train Neural Network with updated architecture\n",
        "            self.models['neural_net'] = self.create_canadian_neural_network(\n",
        "                (X_train_scaled.shape[1],))\n",
        "\n",
        "            # Train models\n",
        "            for name, model in self.models.items():\n",
        "                if model is not None:\n",
        "                    print(f\"\\nTraining {name} model...\")\n",
        "                    try:\n",
        "                        if name == 'neural_net':\n",
        "                            model.fit(\n",
        "                                X_train_scaled, y_train_scaled,\n",
        "                                validation_data=(X_val_scaled, y_val_scaled),\n",
        "                                epochs=100,\n",
        "                                batch_size=32,\n",
        "                                verbose=0,\n",
        "                                callbacks=[\n",
        "                                    keras.callbacks.EarlyStopping(\n",
        "                                        monitor='val_loss',\n",
        "                                        patience=10,\n",
        "                                        restore_best_weights=True\n",
        "                                    )\n",
        "                                ]\n",
        "                            )\n",
        "                        elif name == 'lightgbm':\n",
        "                            model.fit(\n",
        "                                X_train_scaled,\n",
        "                                y_train_scaled,\n",
        "                                eval_set=[(X_val_scaled, y_val_scaled)],\n",
        "                                callbacks=[\n",
        "                                    lgb.early_stopping(100),  # Increased patience\n",
        "                                    lgb.log_evaluation(0)\n",
        "                                ]\n",
        "                            )\n",
        "                        elif name == 'xgboost':\n",
        "                            model.fit(\n",
        "                                X_train_scaled,\n",
        "                                y_train_scaled,\n",
        "                                eval_set=[(X_val_scaled, y_val_scaled)],\n",
        "                                early_stopping_rounds=100,  # Increased patience\n",
        "                                verbose=False\n",
        "                            )\n",
        "                        elif name == 'catboost':\n",
        "                            model.fit(\n",
        "                                X_train_scaled,\n",
        "                                y_train_scaled,\n",
        "                                eval_set=(X_val_scaled, y_val_scaled),\n",
        "                                early_stopping_rounds=100,  # Increased patience\n",
        "                                verbose=False\n",
        "                            )\n",
        "\n",
        "                        # Log metrics\n",
        "                        train_pred = self.y_scaler.inverse_transform(\n",
        "                            model.predict(X_train_scaled).reshape(-1, 1)).ravel()\n",
        "                        val_pred = self.y_scaler.inverse_transform(\n",
        "                            model.predict(X_val_scaled).reshape(-1, 1)).ravel()\n",
        "\n",
        "                        train_mae = mean_absolute_error(y_train, train_pred)\n",
        "                        val_mae = mean_absolute_error(y_val, val_pred)\n",
        "                        train_r2 = r2_score(y_train, train_pred)\n",
        "                        val_r2 = r2_score(y_val, val_pred)\n",
        "\n",
        "                        print(f\"{name} Results:\")\n",
        "                        print(f\"Train MAE: {train_mae:.3f}, Train R2: {train_r2:.3f}\")\n",
        "                        print(f\"Val MAE: {val_mae:.3f}, Val R2: {val_r2:.3f}\")\n",
        "\n",
        "                        mlflow.log_metrics({\n",
        "                            f'{name}_train_mae': train_mae,\n",
        "                            f'{name}_val_mae': val_mae,\n",
        "                            f'{name}_train_r2': train_r2,\n",
        "                            f'{name}_val_r2': val_r2\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error training {name} model: {e}\")\n",
        "                        self.models[name] = None  # Mark model as failed\n",
        "\n",
        "    def predict_performance(self, X):\n",
        "        \"\"\"Make predictions using ensemble of models\"\"\"\n",
        "        X_scaled = self.X_scaler.transform(X)\n",
        "        predictions = {}\n",
        "        weights = {\n",
        "            'lightgbm': 0.3,\n",
        "            'xgboost': 0.25,\n",
        "            'catboost': 0.25,\n",
        "            'neural_net': 0.2\n",
        "        }\n",
        "\n",
        "        # Initialize final prediction array\n",
        "        final_pred = np.zeros(X.shape[0])\n",
        "        weight_sum = 0\n",
        "\n",
        "        # Get predictions from each model\n",
        "        for name, model in self.models.items():\n",
        "            if model is not None:\n",
        "                try:\n",
        "                    # Ensure predictions are 1D array\n",
        "                    pred = model.predict(X_scaled)\n",
        "                    if len(pred.shape) > 1:\n",
        "                        pred = pred.ravel()\n",
        "\n",
        "                    weight = weights.get(name, 0)\n",
        "                    final_pred += pred * weight\n",
        "                    weight_sum += weight\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Error getting predictions from {name} model: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if weight_sum > 0:\n",
        "            final_pred /= weight_sum\n",
        "            # Scale back predictions\n",
        "            final_pred = self.y_scaler.inverse_transform(final_pred.reshape(-1, 1)).ravel()\n",
        "            return final_pred\n",
        "        else:\n",
        "            print(\"Warning: No valid predictions from any model\")\n",
        "            return None\n",
        "\n",
        "    def analyze_performance(self, X, feature_names):\n",
        "        \"\"\"Analyze feature importance and performance patterns\"\"\"\n",
        "        if self.models['lightgbm'] is not None:\n",
        "            # SHAP analysis\n",
        "            explainer = shap.TreeExplainer(self.models['lightgbm'])\n",
        "            shap_values = explainer.shap_values(X)\n",
        "\n",
        "            plt.figure(figsize=(15, 10))\n",
        "            shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)\n",
        "            plt.title(\"Feature Importance Analysis\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Feature importance\n",
        "            importance = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': np.abs(shap_values).mean(0)\n",
        "            })\n",
        "            importance = importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "            print(\"\\nTop 10 Most Important Features:\")\n",
        "            print(importance.head(10))\n",
        "\n",
        "    def predict_2025_canadian_gp(self):\n",
        "        \"\"\"Predict the finishing order for the 2025 Canadian GP\"\"\"\n",
        "        try:\n",
        "            print(\"\\nPredicting 2025 Canadian Grand Prix Results...\")\n",
        "\n",
        "            # Create a sample race conditions dataframe\n",
        "            race_conditions = []\n",
        "\n",
        "            # Montreal typical June conditions\n",
        "            base_conditions = {\n",
        "                'TrackTemp': 35,  # Typical June temperature\n",
        "                'AirTemp': 22,\n",
        "                'Humidity': 65,  # Typical Montreal humidity\n",
        "                'WeatherCondition': 'DRY',  # Single value, not repeated\n",
        "                'Session': 'Race',\n",
        "                'Year': 2025,\n",
        "                'LapNumber': 1,\n",
        "                'Stint': 1,\n",
        "                'SpeedI1': 0,  # Will be filled with realistic values\n",
        "                'SpeedI2': 0,\n",
        "                'SpeedFL': 0,\n",
        "                'SpeedST': 0,\n",
        "                'TyreLife': 0,\n",
        "                'Position': 1,\n",
        "                'TrackStatus': '1',\n",
        "                'Compound': 'MEDIUM',\n",
        "                'BrakeWear': 1.0,\n",
        "                'TrackEvolution': 1.0,\n",
        "                'TirePerformance': 1.0,\n",
        "                'DRS': 0,  # DRS availability\n",
        "                'SafetyCar': 0,  # Safety car probability\n",
        "                'TrackGrip': 1.0,  # Track grip level\n",
        "                'WindSpeed': 10,  # Typical wind speed in km/h\n",
        "                'WindDirection': 0,  # Wind direction in degrees\n",
        "                'CloudCover': 30  # Cloud cover percentage\n",
        "            }\n",
        "\n",
        "            # Define typical sector speeds for Montreal (in km/h)\n",
        "            sector_speeds = {\n",
        "                'SpeedI1': (315, 325),  # Speed trap 1 range\n",
        "                'SpeedI2': (300, 310),  # Speed trap 2 range\n",
        "                'SpeedFL': (290, 300),  # Flying lap speed range\n",
        "                'SpeedST': (305, 315)   # Speed trap range\n",
        "            }\n",
        "\n",
        "            # Base lap time for Montreal (in seconds)\n",
        "            base_lap_time = 75.0  # Typical race lap time for Montreal\n",
        "\n",
        "            # Tire strategy options for Montreal\n",
        "            tire_strategies = {\n",
        "                'Red Bull Racing': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Ferrari': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'McLaren': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Mercedes': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Aston Martin': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Audi': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Alpine': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Williams': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Sauber': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'VCARB': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'},\n",
        "                'Haas': {'stint1': 'MEDIUM', 'stint2': 'HARD', 'stint3': 'MEDIUM'}\n",
        "            }\n",
        "\n",
        "            # Generate race simulation data for each driver\n",
        "            for driver_code, info in self.drivers_2025.items():\n",
        "                # Get team and driver performance factors\n",
        "                team_factor = self.team_factors[info['team']]\n",
        "                driver_factor = info['performance_factor']\n",
        "                team_strategy = tire_strategies[info['team']]\n",
        "\n",
        "                # Simulate multiple laps for each driver\n",
        "                for lap in range(1, 71):  # 70 laps\n",
        "                    conditions = base_conditions.copy()\n",
        "\n",
        "                    # Calculate stint and compound based on strategy\n",
        "                    stint = min((lap - 1) // 23 + 1, 3)  # Cap at 3 stints\n",
        "                    compound = team_strategy[f'stint{stint}']\n",
        "\n",
        "                    # Calculate base lap time with team and driver factors\n",
        "                    lap_time = base_lap_time * team_factor * driver_factor\n",
        "\n",
        "                    # Apply tire wear effect\n",
        "                    tire_wear = 1 + (conditions['TyreLife'] * 0.001)  # 0.1% degradation per lap\n",
        "                    lap_time *= tire_wear\n",
        "\n",
        "                    # Apply compound effect\n",
        "                    compound_factor = 1.0\n",
        "                    if compound == 'HARD':\n",
        "                        compound_factor = 1.02  # Hard tires are 2% slower\n",
        "                    elif compound == 'SOFT':\n",
        "                        compound_factor = 0.98  # Soft tires are 2% faster\n",
        "                    lap_time *= compound_factor\n",
        "\n",
        "                    # Apply track evolution\n",
        "                    track_evolution = 1.0 - (lap / 70) * 0.05  # Up to 5% improvement\n",
        "                    lap_time *= track_evolution\n",
        "\n",
        "                    # Apply DRS effect if available\n",
        "                    if conditions['DRS']:\n",
        "                        lap_time *= 0.98  # 2% faster with DRS\n",
        "\n",
        "                    # Add small random variation (±0.2%)\n",
        "                    lap_time *= (0.998 + np.random.random() * 0.004)\n",
        "\n",
        "                    # Update conditions with calculated lap time\n",
        "                    conditions.update({\n",
        "                        'Driver': driver_code,\n",
        "                        'Team': info['team'],\n",
        "                        'LapNumber': lap,\n",
        "                        'Stint': stint,\n",
        "                        'Compound': compound,\n",
        "                        'TyreLife': lap % 23,  # Reset after each pit stop\n",
        "                        'TrackStatus': '1',  # Normal racing conditions\n",
        "                        'Position': 1,  # Will be adjusted based on predictions\n",
        "                        'TrackEvolution': track_evolution,\n",
        "                        'BrakeWear': 1.0 - (lap / 70) * 0.15,  # Brake degradation\n",
        "                        'DRS': 1 if lap > 2 else 0,  # DRS available after lap 2\n",
        "                        'SafetyCar': 0.05 if lap > 10 else 0,  # Small chance of SC after lap 10\n",
        "                        'TrackGrip': 1.0 + (lap / 70) * 0.05,  # Track grip improves slightly\n",
        "                        'LapTime_seconds': lap_time  # Add the calculated lap time\n",
        "                    })\n",
        "\n",
        "                    # Add realistic speed variations with team and driver factors\n",
        "                    for speed_col, (min_speed, max_speed) in sector_speeds.items():\n",
        "                        base_speed = (min_speed + max_speed) / 2\n",
        "                        # Add some random variation and account for tire wear\n",
        "                        speed_factor = 1.0 - (conditions['TyreLife'] * 0.002)  # 0.2% degradation per lap\n",
        "                        # Apply team and driver performance factors\n",
        "                        speed_factor *= team_factor * driver_factor\n",
        "                        # Apply DRS effect if available\n",
        "                        if conditions['DRS']:\n",
        "                            speed_factor *= 1.03  # 3% speed boost with DRS\n",
        "                        conditions[speed_col] = base_speed * speed_factor * (0.98 + np.random.random() * 0.04)\n",
        "\n",
        "                    race_conditions.append(conditions)\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            race_df = pd.DataFrame(race_conditions)\n",
        "\n",
        "            # Preprocess the race simulation data\n",
        "            processed_data = self.preprocess_data(race_df)\n",
        "            if processed_data is None:\n",
        "                raise ValueError(\"Failed to preprocess race simulation data\")\n",
        "\n",
        "            # Ensure columns match training data\n",
        "            if not hasattr(self, 'training_columns'):\n",
        "                raise ValueError(\"Model hasn't been trained yet. No training columns available.\")\n",
        "\n",
        "            # Add missing columns with zeros using pd.concat to avoid fragmentation\n",
        "            missing_cols = set(self.training_columns) - set(processed_data.columns)\n",
        "            if missing_cols:\n",
        "                missing_data = pd.DataFrame(0, index=processed_data.index, columns=list(missing_cols))\n",
        "                processed_data = pd.concat([processed_data, missing_data], axis=1)\n",
        "\n",
        "            # Remove extra columns\n",
        "            processed_data = processed_data[self.training_columns]\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = self.predict_performance(processed_data)\n",
        "            if predictions is None:\n",
        "                raise ValueError(\"Failed to generate predictions\")\n",
        "\n",
        "            # Calculate average lap time for each driver\n",
        "            driver_performances = {}\n",
        "            for i, pred in enumerate(predictions):\n",
        "                driver = race_conditions[i]['Driver']\n",
        "                if driver not in driver_performances:\n",
        "                    driver_performances[driver] = []\n",
        "                driver_performances[driver].append(pred)\n",
        "\n",
        "            # Calculate average performance (excluding outliers)\n",
        "            final_performances = {}\n",
        "            for driver, times in driver_performances.items():\n",
        "                times_array = np.array(times)\n",
        "                # Remove outliers (times outside 1.5 IQR)\n",
        "                Q1 = np.percentile(times_array, 25)\n",
        "                Q3 = np.percentile(times_array, 75)\n",
        "                IQR = Q3 - Q1\n",
        "                mask = (times_array >= Q1 - 1.5 * IQR) & (times_array <= Q3 + 1.5 * IQR)\n",
        "                final_performances[driver] = np.mean(times_array[mask])\n",
        "\n",
        "            # Sort drivers by performance\n",
        "            sorted_drivers = sorted(final_performances.items(), key=lambda x: x[1])\n",
        "\n",
        "            # Print predictions\n",
        "            print(\"\\n2025 Canadian Grand Prix - Predicted Top 10:\")\n",
        "            print(\"\\nPos  Driver                  Team                  Predicted Avg Lap\")\n",
        "            print(\"-\" * 65)\n",
        "\n",
        "            for pos, (driver_code, avg_time) in enumerate(sorted_drivers[:10], 1):\n",
        "                driver_info = self.drivers_2025[driver_code]\n",
        "                print(f\"{pos:2d}.  {driver_info['name']:<20s} {driver_info['team']:<20s} {avg_time:.3f}s\")\n",
        "\n",
        "            return sorted_drivers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting 2025 race results: {e}\")\n",
        "            return None\n",
        "\n"
      ],
      "metadata": {
        "id": "IFNCiCII-MM0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize predictor\n",
        "    predictor = CanadianGPPredictor()\n",
        "\n",
        "    # Load historical data\n",
        "    print(\"Loading historical data...\")\n",
        "    data = predictor.load_historical_data()\n",
        "    if data is None:\n",
        "        print(\"Error: Could not load historical data\")\n",
        "        return\n",
        "\n",
        "    # Preprocess data\n",
        "    print(\"\\nPreprocessing data...\")\n",
        "    processed_data = predictor.preprocess_data(data)\n",
        "    if processed_data is None:\n",
        "        print(\"Error: Data preprocessing failed\")\n",
        "        return\n",
        "\n",
        "    # Prepare features and target\n",
        "    y = processed_data['LapTime_seconds']\n",
        "    X = processed_data.drop(['LapTime_seconds'], axis=1)\n",
        "\n",
        "    # Split data\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    for train_idx, val_idx in tscv.split(X):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Train models\n",
        "        predictor.train_models(X_train, y_train, X_val, y_val)\n",
        "        break  # Only use first split for now\n",
        "\n",
        "    # Make predictions for 2025 Canadian GP\n",
        "    print(\"\\nGenerating predictions for 2025 Canadian Grand Prix...\")\n",
        "    predictor.predict_2025_canadian_gp()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGgJt2-r-Tc4",
        "outputId": "4b2150de-90ea-4503-c423-3783cc796482"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "core           INFO \tLoading data for Canadian Grand Prix - Race [v3.5.3]\n",
            "INFO:fastf1.fastf1.core:Loading data for Canadian Grand Prix - Race [v3.5.3]\n",
            "req            INFO \tUsing cached data for session_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_info\n",
            "req            INFO \tUsing cached data for driver_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for driver_info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading historical data...\n",
            "\n",
            "Loading 2024 data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "req            INFO \tUsing cached data for session_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_status_data\n",
            "req            INFO \tUsing cached data for lap_count\n",
            "INFO:fastf1.fastf1.req:Using cached data for lap_count\n",
            "req            INFO \tUsing cached data for track_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for track_status_data\n",
            "req            INFO \tUsing cached data for _extended_timing_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for _extended_timing_data\n",
            "req            INFO \tUsing cached data for timing_app_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for timing_app_data\n",
            "core           INFO \tProcessing timing data...\n",
            "INFO:fastf1.fastf1.core:Processing timing data...\n",
            "req            INFO \tUsing cached data for car_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for car_data\n",
            "req            INFO \tUsing cached data for position_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for position_data\n",
            "req            INFO \tUsing cached data for weather_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for weather_data\n",
            "req            INFO \tUsing cached data for race_control_messages\n",
            "INFO:fastf1.fastf1.req:Using cached data for race_control_messages\n",
            "core           INFO \tFinished loading data for 20 drivers: ['1', '4', '63', '44', '81', '14', '18', '3', '10', '31', '27', '20', '77', '22', '24', '55', '23', '11', '16', '2']\n",
            "INFO:fastf1.fastf1.core:Finished loading data for 20 drivers: ['1', '4', '63', '44', '81', '14', '18', '3', '10', '31', '27', '20', '77', '22', '24', '55', '23', '11', '16', '2']\n",
            "core           INFO \tLoading data for Canadian Grand Prix - Qualifying [v3.5.3]\n",
            "INFO:fastf1.fastf1.core:Loading data for Canadian Grand Prix - Qualifying [v3.5.3]\n",
            "req            INFO \tUsing cached data for session_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_info\n",
            "req            INFO \tUsing cached data for driver_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for driver_info\n",
            "req            INFO \tUsing cached data for session_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_status_data\n",
            "req            INFO \tUsing cached data for track_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for track_status_data\n",
            "req            INFO \tUsing cached data for _extended_timing_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for _extended_timing_data\n",
            "req            INFO \tUsing cached data for timing_app_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for timing_app_data\n",
            "core           INFO \tProcessing timing data...\n",
            "INFO:fastf1.fastf1.core:Processing timing data...\n",
            "req            INFO \tUsing cached data for car_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for car_data\n",
            "req            INFO \tUsing cached data for position_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for position_data\n",
            "req            INFO \tUsing cached data for weather_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for weather_data\n",
            "req            INFO \tUsing cached data for race_control_messages\n",
            "INFO:fastf1.fastf1.req:Using cached data for race_control_messages\n",
            "core           INFO \tFinished loading data for 20 drivers: ['63', '1', '4', '81', '3', '14', '44', '22', '18', '23', '16', '55', '2', '20', '10', '11', '77', '31', '27', '24']\n",
            "INFO:fastf1.fastf1.core:Finished loading data for 20 drivers: ['63', '1', '4', '81', '3', '14', '44', '22', '18', '23', '16', '55', '2', '20', '10', '11', '77', '31', '27', '24']\n",
            "core           INFO \tLoading data for Canadian Grand Prix - Race [v3.5.3]\n",
            "INFO:fastf1.fastf1.core:Loading data for Canadian Grand Prix - Race [v3.5.3]\n",
            "req            INFO \tUsing cached data for session_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_info\n",
            "req            INFO \tUsing cached data for driver_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for driver_info\n",
            "req            INFO \tUsing cached data for session_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_status_data\n",
            "req            INFO \tUsing cached data for lap_count\n",
            "INFO:fastf1.fastf1.req:Using cached data for lap_count\n",
            "req            INFO \tUsing cached data for track_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for track_status_data\n",
            "req            INFO \tUsing cached data for _extended_timing_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for _extended_timing_data\n",
            "req            INFO \tUsing cached data for timing_app_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for timing_app_data\n",
            "core           INFO \tProcessing timing data...\n",
            "INFO:fastf1.fastf1.core:Processing timing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No sprint data for 2024: Session type 'S' does not exist for this event\n",
            "\n",
            "Loading 2023 data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "req            INFO \tUsing cached data for car_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for car_data\n",
            "req            INFO \tUsing cached data for position_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for position_data\n",
            "req            INFO \tUsing cached data for weather_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for weather_data\n",
            "req            INFO \tUsing cached data for race_control_messages\n",
            "INFO:fastf1.fastf1.req:Using cached data for race_control_messages\n",
            "core           INFO \tFinished loading data for 20 drivers: ['1', '14', '44', '16', '55', '11', '23', '31', '18', '77', '81', '10', '4', '22', '27', '24', '20', '21', '63', '2']\n",
            "INFO:fastf1.fastf1.core:Finished loading data for 20 drivers: ['1', '14', '44', '16', '55', '11', '23', '31', '18', '77', '81', '10', '4', '22', '27', '24', '20', '21', '63', '2']\n",
            "core           INFO \tLoading data for Canadian Grand Prix - Qualifying [v3.5.3]\n",
            "INFO:fastf1.fastf1.core:Loading data for Canadian Grand Prix - Qualifying [v3.5.3]\n",
            "req            INFO \tUsing cached data for session_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_info\n",
            "req            INFO \tUsing cached data for driver_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for driver_info\n",
            "DEBUG:fastf1.ergast:Failed to parse timestamp '' in Ergastresponse.\n",
            "req            INFO \tUsing cached data for session_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_status_data\n",
            "req            INFO \tUsing cached data for track_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for track_status_data\n",
            "req            INFO \tUsing cached data for _extended_timing_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for _extended_timing_data\n",
            "req            INFO \tUsing cached data for timing_app_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for timing_app_data\n",
            "core           INFO \tProcessing timing data...\n",
            "INFO:fastf1.fastf1.core:Processing timing data...\n",
            "core        WARNING \tDriver  1: Lap timing integrity check failed for 1 lap(s)\n",
            "WARNING:fastf1.fastf1.core:Driver  1: Lap timing integrity check failed for 1 lap(s)\n",
            "core        WARNING \tDriver 55: Lap timing integrity check failed for 1 lap(s)\n",
            "WARNING:fastf1.fastf1.core:Driver 55: Lap timing integrity check failed for 1 lap(s)\n",
            "core        WARNING \tDriver 11: Lap timing integrity check failed for 1 lap(s)\n",
            "WARNING:fastf1.fastf1.core:Driver 11: Lap timing integrity check failed for 1 lap(s)\n",
            "core        WARNING \tDriver  2: Lap timing integrity check failed for 1 lap(s)\n",
            "WARNING:fastf1.fastf1.core:Driver  2: Lap timing integrity check failed for 1 lap(s)\n",
            "req            INFO \tUsing cached data for car_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for car_data\n",
            "req            INFO \tUsing cached data for position_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for position_data\n",
            "req            INFO \tUsing cached data for weather_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for weather_data\n",
            "req            INFO \tUsing cached data for race_control_messages\n",
            "INFO:fastf1.fastf1.req:Using cached data for race_control_messages\n",
            "core           INFO \tFinished loading data for 20 drivers: ['1', '27', '14', '44', '63', '31', '4', '55', '81', '23', '16', '11', '18', '20', '77', '22', '10', '21', '2', '24']\n",
            "INFO:fastf1.fastf1.core:Finished loading data for 20 drivers: ['1', '27', '14', '44', '63', '31', '4', '55', '81', '23', '16', '11', '18', '20', '77', '22', '10', '21', '2', '24']\n",
            "core           INFO \tLoading data for Canadian Grand Prix - Race [v3.5.3]\n",
            "INFO:fastf1.fastf1.core:Loading data for Canadian Grand Prix - Race [v3.5.3]\n",
            "req            INFO \tUsing cached data for session_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_info\n",
            "req            INFO \tUsing cached data for driver_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for driver_info\n",
            "req            INFO \tUsing cached data for session_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_status_data\n",
            "req            INFO \tUsing cached data for lap_count\n",
            "INFO:fastf1.fastf1.req:Using cached data for lap_count\n",
            "req            INFO \tUsing cached data for track_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for track_status_data\n",
            "req            INFO \tUsing cached data for _extended_timing_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for _extended_timing_data\n",
            "req            INFO \tUsing cached data for timing_app_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for timing_app_data\n",
            "core           INFO \tProcessing timing data...\n",
            "INFO:fastf1.fastf1.core:Processing timing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No sprint data for 2023: Session type 'S' does not exist for this event\n",
            "\n",
            "Loading 2022 data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "req            INFO \tUsing cached data for car_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for car_data\n",
            "req            INFO \tUsing cached data for position_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for position_data\n",
            "req            INFO \tUsing cached data for weather_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for weather_data\n",
            "req            INFO \tUsing cached data for race_control_messages\n",
            "INFO:fastf1.fastf1.req:Using cached data for race_control_messages\n",
            "core           INFO \tFinished loading data for 20 drivers: ['1', '55', '44', '63', '16', '31', '77', '24', '14', '18', '3', '5', '23', '10', '4', '6', '20', '22', '47', '11']\n",
            "INFO:fastf1.fastf1.core:Finished loading data for 20 drivers: ['1', '55', '44', '63', '16', '31', '77', '24', '14', '18', '3', '5', '23', '10', '4', '6', '20', '22', '47', '11']\n",
            "core           INFO \tLoading data for Canadian Grand Prix - Qualifying [v3.5.3]\n",
            "INFO:fastf1.fastf1.core:Loading data for Canadian Grand Prix - Qualifying [v3.5.3]\n",
            "req            INFO \tUsing cached data for session_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_info\n",
            "req            INFO \tUsing cached data for driver_info\n",
            "INFO:fastf1.fastf1.req:Using cached data for driver_info\n",
            "req            INFO \tUsing cached data for session_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for session_status_data\n",
            "req            INFO \tUsing cached data for track_status_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for track_status_data\n",
            "req            INFO \tUsing cached data for _extended_timing_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for _extended_timing_data\n",
            "req            INFO \tUsing cached data for timing_app_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for timing_app_data\n",
            "core           INFO \tProcessing timing data...\n",
            "INFO:fastf1.fastf1.core:Processing timing data...\n",
            "req            INFO \tUsing cached data for car_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for car_data\n",
            "req            INFO \tUsing cached data for position_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for position_data\n",
            "req            INFO \tUsing cached data for weather_data\n",
            "INFO:fastf1.fastf1.req:Using cached data for weather_data\n",
            "req            INFO \tUsing cached data for race_control_messages\n",
            "INFO:fastf1.fastf1.req:Using cached data for race_control_messages\n",
            "core           INFO \tFinished loading data for 20 drivers: ['1', '14', '55', '44', '20', '47', '31', '63', '3', '24', '77', '23', '11', '4', '16', '10', '5', '18', '6', '22']\n",
            "INFO:fastf1.fastf1.core:Finished loading data for 20 drivers: ['1', '14', '55', '44', '20', '47', '31', '63', '3', '24', '77', '23', '11', '4', '16', '10', '5', '18', '6', '22']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No sprint data for 2022: Session type 'S' does not exist for this event\n",
            "\n",
            "Unique tire compounds found: ['INTERMEDIATE' 'MEDIUM' 'HARD' 'WET' 'SOFT']\n",
            "\n",
            "Preprocessing data...\n",
            "\n",
            "Starting data preprocessing...\n",
            "Initial columns: ['Time', 'Driver', 'DriverNumber', 'LapTime', 'LapNumber', 'Stint', 'PitOutTime', 'PitInTime', 'Sector1Time', 'Sector2Time', 'Sector3Time', 'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'IsPersonalBest', 'Compound', 'TyreLife', 'FreshTyre', 'Team', 'LapStartTime', 'LapStartDate', 'TrackStatus', 'Position', 'Deleted', 'DeletedReason', 'FastF1Generated', 'IsAccurate', 'Session', 'Year', 'TirePerformance', 'TrackEvolution', 'BrakeWear']\n",
            "\n",
            "Processing categorical column: Driver\n",
            "Unique values in Driver: ['VER' 'GAS' 'PER' 'ALO' 'LEC' 'STR' 'SAR' 'MAG' 'TSU' 'ALB' 'ZHO' 'HUL'\n",
            " 'RIC' 'OCO' 'NOR' 'HAM' 'SAI' 'RUS' 'BOT' 'PIA' 'DEV' 'MSC' 'VET' 'LAT']\n",
            "Successfully created dummies for Driver\n",
            "\n",
            "Processing categorical column: Team\n",
            "Unique values in Team: ['Red Bull Racing' 'Alpine' 'Aston Martin' 'Ferrari' 'Williams'\n",
            " 'Haas F1 Team' 'RB' 'Kick Sauber' 'McLaren' 'Mercedes' 'AlphaTauri'\n",
            " 'Alfa Romeo']\n",
            "Successfully created dummies for Team\n",
            "\n",
            "Processing categorical column: Compound\n",
            "Unique values in Compound: ['INTERMEDIATE' 'MEDIUM' 'HARD' 'WET' 'SOFT']\n",
            "Successfully created dummies for Compound\n",
            "\n",
            "Processing categorical column: TrackStatus\n",
            "Unique values in TrackStatus: ['12' '1' '124' '4' '412' '24' '41' '21' '2671' '126' '671' '125' '251'\n",
            " '267' '71' '6' '26' '215' '5' '51']\n",
            "Successfully created dummies for TrackStatus\n",
            "\n",
            "Processing categorical column: Session\n",
            "Unique values in Session: ['Race' 'Qualifying']\n",
            "Successfully created dummies for Session\n",
            "\n",
            "Final columns after preprocessing: ['LapNumber', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife', 'Position', 'Year', 'TirePerformance', 'TrackEvolution', 'BrakeWear', 'IsRace', 'IsQuali', 'IsSprint', 'Time_seconds', 'LapTime_seconds', 'PitOutTime_seconds', 'PitInTime_seconds', 'Sector1Time_seconds', 'Sector2Time_seconds', 'Sector3Time_seconds', 'Sector1SessionTime_seconds', 'Sector2SessionTime_seconds', 'Sector3SessionTime_seconds', 'LapStartTime_seconds', 'DayOfYear', 'MonthOfYear', 'DayOfWeek', 'Driver_ALB', 'Driver_ALO', 'Driver_BOT', 'Driver_DEV', 'Driver_GAS', 'Driver_HAM', 'Driver_HUL', 'Driver_LAT', 'Driver_LEC', 'Driver_MAG', 'Driver_MSC', 'Driver_NOR', 'Driver_OCO', 'Driver_PER', 'Driver_PIA', 'Driver_RIC', 'Driver_RUS', 'Driver_SAI', 'Driver_SAR', 'Driver_STR', 'Driver_TSU', 'Driver_VER', 'Driver_VET', 'Driver_ZHO', 'Team_Alfa Romeo', 'Team_AlphaTauri', 'Team_Alpine', 'Team_Aston Martin', 'Team_Ferrari', 'Team_Haas F1 Team', 'Team_Kick Sauber', 'Team_McLaren', 'Team_Mercedes', 'Team_RB', 'Team_Red Bull Racing', 'Team_Williams', 'Compound_HARD', 'Compound_INTERMEDIATE', 'Compound_MEDIUM', 'Compound_SOFT', 'Compound_WET', 'TrackStatus_1', 'TrackStatus_12', 'TrackStatus_124', 'TrackStatus_125', 'TrackStatus_126', 'TrackStatus_21', 'TrackStatus_215', 'TrackStatus_24', 'TrackStatus_251', 'TrackStatus_26', 'TrackStatus_267', 'TrackStatus_2671', 'TrackStatus_4', 'TrackStatus_41', 'TrackStatus_412', 'TrackStatus_5', 'TrackStatus_51', 'TrackStatus_6', 'TrackStatus_671', 'TrackStatus_71', 'Session_Qualifying', 'Session_Race']\n",
            "\n",
            "Numeric columns: ['LapNumber', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife', 'Position', 'Year', 'TirePerformance', 'TrackEvolution', 'BrakeWear', 'IsRace', 'IsQuali', 'IsSprint', 'Time_seconds', 'LapTime_seconds', 'PitOutTime_seconds', 'PitInTime_seconds', 'Sector1Time_seconds', 'Sector2Time_seconds', 'Sector3Time_seconds', 'Sector1SessionTime_seconds', 'Sector2SessionTime_seconds', 'Sector3SessionTime_seconds', 'LapStartTime_seconds', 'DayOfYear', 'MonthOfYear', 'DayOfWeek']\n",
            "\n",
            "Missing values after processing:\n",
            "Lap([], dtype: int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training lightgbm model...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's rmse: 0.348124\n",
            "lightgbm Results:\n",
            "Train MAE: 0.145, Train R2: 0.997\n",
            "Val MAE: 2.370, Val R2: 0.908\n",
            "\n",
            "Training xgboost model...\n",
            "Error training xgboost model: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
            "\n",
            "Training catboost model...\n",
            "catboost Results:\n",
            "Train MAE: 0.409, Train R2: 0.998\n",
            "Val MAE: 4.580, Val R2: 0.732\n",
            "\n",
            "Training neural_net model...\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "neural_net Results:\n",
            "Train MAE: 1.841, Train R2: 0.963\n",
            "Val MAE: 4.543, Val R2: 0.779\n",
            "\n",
            "Generating predictions for 2025 Canadian Grand Prix...\n",
            "\n",
            "Predicting 2025 Canadian Grand Prix Results...\n",
            "\n",
            "Starting data preprocessing...\n",
            "Initial columns: ['TrackTemp', 'AirTemp', 'Humidity', 'WeatherCondition', 'Session', 'Year', 'LapNumber', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife', 'Position', 'TrackStatus', 'Compound', 'BrakeWear', 'TrackEvolution', 'TirePerformance', 'DRS', 'SafetyCar', 'TrackGrip', 'WindSpeed', 'WindDirection', 'CloudCover', 'Driver', 'Team', 'LapTime_seconds']\n",
            "\n",
            "Processing categorical column: Driver\n",
            "Unique values in Driver: ['VER' 'NOR' 'PIA' 'RUS' 'SAI' 'ALB' 'LEC' 'OCO' 'HAM' 'STR' 'GAS' 'ALO'\n",
            " 'HUL']\n",
            "Successfully created dummies for Driver\n",
            "\n",
            "Processing categorical column: Team\n",
            "Unique values in Team: ['Red Bull Racing' 'McLaren' 'Mercedes' 'Audi' 'Williams' 'Ferrari'\n",
            " 'Alpine' 'Aston Martin' 'Haas']\n",
            "Successfully created dummies for Team\n",
            "\n",
            "Processing categorical column: Compound\n",
            "Unique values in Compound: ['MEDIUM' 'HARD']\n",
            "Successfully created dummies for Compound\n",
            "\n",
            "Processing categorical column: TrackStatus\n",
            "Unique values in TrackStatus: ['1']\n",
            "Successfully created dummies for TrackStatus\n",
            "\n",
            "Processing categorical column: Session\n",
            "Unique values in Session: ['Race']\n",
            "Successfully created dummies for Session\n",
            "\n",
            "Processing categorical column: WeatherCondition\n",
            "Unique values in WeatherCondition: ['DRY']\n",
            "Successfully created dummies for WeatherCondition\n",
            "\n",
            "Final columns after preprocessing: ['TrackTemp', 'AirTemp', 'Humidity', 'Year', 'LapNumber', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife', 'Position', 'BrakeWear', 'TrackEvolution', 'TirePerformance', 'DRS', 'SafetyCar', 'TrackGrip', 'WindSpeed', 'WindDirection', 'CloudCover', 'LapTime_seconds', 'IsRace', 'IsQuali', 'IsSprint', 'Driver_ALB', 'Driver_ALO', 'Driver_GAS', 'Driver_HAM', 'Driver_HUL', 'Driver_LEC', 'Driver_NOR', 'Driver_OCO', 'Driver_PIA', 'Driver_RUS', 'Driver_SAI', 'Driver_STR', 'Driver_VER', 'Team_Alpine', 'Team_Aston Martin', 'Team_Audi', 'Team_Ferrari', 'Team_Haas', 'Team_McLaren', 'Team_Mercedes', 'Team_Red Bull Racing', 'Team_Williams', 'Compound_HARD', 'Compound_MEDIUM', 'TrackStatus_1', 'Session_Race', 'WeatherCondition_DRY']\n",
            "\n",
            "Numeric columns: ['TrackTemp', 'AirTemp', 'Humidity', 'Year', 'LapNumber', 'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife', 'Position', 'BrakeWear', 'TrackEvolution', 'TirePerformance', 'DRS', 'SafetyCar', 'TrackGrip', 'WindSpeed', 'WindDirection', 'CloudCover', 'LapTime_seconds', 'IsRace', 'IsQuali', 'IsSprint']\n",
            "\n",
            "Missing values after processing:\n",
            "Series([], dtype: int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\n",
            "2025 Canadian Grand Prix - Predicted Top 10:\n",
            "\n",
            "Pos  Driver                  Team                  Predicted Avg Lap\n",
            "-----------------------------------------------------------------\n",
            " 1.  Alexander Albon      Williams             125.252s\n",
            " 2.  Max Verstappen       Red Bull Racing      126.000s\n",
            " 3.  Esteban Ocon         Alpine               126.007s\n",
            " 4.  Carlos Sainz         Audi                 126.017s\n",
            " 5.  Pierre Gasly         Alpine               126.151s\n",
            " 6.  Lewis Hamilton       Ferrari              126.243s\n",
            " 7.  Charles Leclerc      Ferrari              126.261s\n",
            " 8.  Lando Norris         McLaren              126.307s\n",
            " 9.  Lance Stroll         Aston Martin         126.428s\n",
            "10.  Oscar Piastri        McLaren              126.489s\n"
          ]
        }
      ]
    }
  ]
}